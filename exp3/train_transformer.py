#!/usr/bin/env python3
"""
Transformerè®­ç»ƒè„šæœ¬ - æ”¯æŒåŒç¯å¢ƒå’Œä¸‰å¤§æ ¸å¿ƒæœºåˆ¶
å¼€å‘ç¯å¢ƒï¼šRTX 5070Ti
è®­ç»ƒç¯å¢ƒï¼šV100s 32GB
"""

import os
import sys
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from typing import Dict, Optional, Tuple, List
import numpy as np
from tqdm import tqdm
import logging

# é¡¹ç›®å¯¼å…¥
from models.model import create_poetry_transformer, SimplifiedPoetryTransformer
from models.config import Config, config
from models.dataset import create_dataloaders
from utils.train_utils import (
    create_optimizer, create_scheduler, 
    save_checkpoint, load_checkpoint,
    calculate_metrics, setup_logging
)
from utils.generate_utils import ForcedLengthDecoding, SimplePoetryGenerator
from utils.visualization import create_training_visualizer

class PoetryTransformerTrainer:
    """è¯—æ­ŒTransformerè®­ç»ƒå™¨ - é›†æˆä¸‰å¤§æ ¸å¿ƒæœºåˆ¶"""
    
    def __init__(self, user_config: Dict, resume_from: Optional[str] = None):
        # ä½¿ç”¨å…¨å±€configä½œä¸ºåŸºç¡€ï¼Œç”¨æˆ·é…ç½®è¦†ç›–
        self.config = config.get_training_config()
        self.config.update(user_config)  # ç”¨æˆ·è®¾ç½®è¦†ç›–é»˜è®¤å€¼
        
        self.device = self._setup_device()
        self.logger = self._setup_logging()
        
        # ä½¿ç”¨config.pyçš„ç¯å¢ƒæ£€æµ‹
        self.environment = config.ENVIRONMENT
        self._apply_environment_optimizations()
        
        # æ¨¡å‹åˆå§‹åŒ–
        self.model = self._create_model()
        self.model.to(self.device)
        
        # è®­ç»ƒç»„ä»¶
        self.optimizer = create_optimizer(self.model, self.config)
        self.scheduler = create_scheduler(self.optimizer, self.config)
        self.criterion = nn.CrossEntropyLoss(ignore_index=self.config['pad_token_id'])
        
        # æ•°æ®åŠ è½½å™¨
        self.train_loader, self.val_loader, self.vocab_info = self._create_dataloaders()
        
        # ç”Ÿæˆå™¨ï¼ˆç”¨äºéªŒè¯ï¼‰
        self.generator = SimplePoetryGenerator(
            self.model, self.vocab_info['ix2word'], self.vocab_info['word2ix']
        )
        
        # å¼ºåˆ¶å¥é•¿è§£ç å™¨
        self.forced_decoder = ForcedLengthDecoding(self.vocab_info['word2ix'])
        
        # è®­ç»ƒçŠ¶æ€
        self.current_epoch = 0
        self.best_loss = float('inf')
        self.training_stats = []
        
        # å¯è§†åŒ–å™¨
        self.visualizer = create_training_visualizer("plots")
        
        # è®­ç»ƒæŒ‡æ ‡è®°å½•
        self.metrics_history = {
            'train_loss': [],
            'val_loss': [],
            'learning_rate': [],
            'epochs': []
        }
        
        # æ¢å¤è®­ç»ƒ
        if resume_from:
            self._resume_training(resume_from)
            
        self.logger.info(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ - ç¯å¢ƒ: {self.environment}")
        self.logger.info(f"æ¨¡å‹å‚æ•°é‡: {self.model.get_num_params():,}")
    
    def _setup_device(self) -> torch.device:
        """è®¾ç½®è®¡ç®—è®¾å¤‡"""
        if torch.cuda.is_available():
            device = torch.device('cuda')
            gpu_name = torch.cuda.get_device_name(0)
            print(f"ğŸš€ ä½¿ç”¨GPU: {gpu_name}")
            print(f"ğŸ’¾ GPUå†…å­˜: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
            return device
        else:
            print("âš ï¸  ä½¿ç”¨CPUè®­ç»ƒ")
            return torch.device('cpu')
    
    def _setup_logging(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—"""
        return setup_logging(
            log_file=f"logs/transformer_training_{time.strftime('%Y%m%d_%H%M%S')}.log"
        )
    
    def _apply_environment_optimizations(self):
        """åº”ç”¨ç¯å¢ƒä¼˜åŒ–é…ç½®"""
        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9 if torch.cuda.is_available() else 0
        gpu_name = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "CPU"
        
        print(f"ğŸš€ ä½¿ç”¨GPU: {gpu_name}")
        print(f"ğŸ’¾ GPUå†…å­˜: {gpu_memory:.1f}GB")
        print(f"ğŸ–¥ï¸  æ£€æµ‹åˆ°{self.environment}ç¯å¢ƒ")
        
        # æ ¹æ®ç¯å¢ƒä¼˜åŒ–é…ç½®
        if self.environment == 'server':
            # V100sæœåŠ¡å™¨ä¼˜åŒ–
            optimizations = {
                'batch_size': 256,  # å¤§å¹…æå‡æ‰¹æ¬¡å¤§å°
                'accumulation_steps': 1,
                'max_seq_len': config.MAX_SEQ_LEN,
                'num_workers': 4,  # ä¼˜åŒ–å·¥ä½œè¿›ç¨‹æ•°ï¼Œé¿å…è¿‡åº¦ä¸Šä¸‹æ–‡åˆ‡æ¢
                'enable_amp': True,  # æ··åˆç²¾åº¦è®­ç»ƒ
                'pin_memory': True,
            }
            print(f"âš¡ æœåŠ¡å™¨ä¼˜åŒ–: æ‰¹æ¬¡{optimizations['batch_size']}, æ··åˆç²¾åº¦è®­ç»ƒ, {optimizations['num_workers']}å·¥ä½œè¿›ç¨‹")
            print("ğŸ”¥ æ€§èƒ½ä¼˜åŒ–: ç§»é™¤é‡å¤éŸµå¾‹è®¡ç®—ï¼Œå‘é‡åŒ–æ•°æ®å¤„ç†")
            
        elif self.environment == 'development':
            optimizations = {
                'batch_size': 64,
                'accumulation_steps': 1,
                'max_seq_len': config.MAX_SEQ_LEN,
                'num_workers': 4,
                'enable_amp': True,
                'pin_memory': True,
            }
            print(f"ğŸ’» å¼€å‘ç¯å¢ƒä¼˜åŒ–: æ‰¹æ¬¡{optimizations['batch_size']}, æ··åˆç²¾åº¦è®­ç»ƒ")
            
        else:
            optimizations = {
                'batch_size': 32,
                'accumulation_steps': 2,
                'max_seq_len': 100,
                'num_workers': 2,
                'enable_amp': torch.cuda.is_available(),
                'pin_memory': False,
            }
            print(f"ğŸ”§ é€šç”¨ç¯å¢ƒé…ç½®: æ‰¹æ¬¡{optimizations['batch_size']}")
        
        # æ›´æ–°é…ç½®
        self.config.update(optimizations)
        
        # åˆå§‹åŒ–æ··åˆç²¾åº¦è®­ç»ƒ
        if self.config.get('enable_amp', False):
            self.scaler = torch.cuda.amp.GradScaler()
            print("âœ… æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
        else:
            self.scaler = None
    
    def _create_model(self) -> SimplifiedPoetryTransformer:
        """åˆ›å»ºæ¨¡å‹ï¼ˆä½¿ç”¨config.pyé…ç½®ï¼‰"""
        # ä½¿ç”¨config.pyä¸­çš„æ¨¡å‹é…ç½®
        model_config = config.get_model_config()
        
        # æ ¹æ®å®é™…æ•°æ®è°ƒæ•´vocab_size
        model_config['vocab_size'] = 8293
        
        print("=" * 60)
        print("SIMPLIFIED POETRY TRANSFORMER MODEL INFO")
        print("=" * 60)
        print("æ¨¡å‹é…ç½®:")
        for key, value in model_config.items():
            print(f"  {key}: {value}")
        
        model = create_poetry_transformer(model_config)
        param_count = model.get_num_params()
        print(f"\næ€»å‚æ•°é‡: {param_count:,} ({param_count/1e6:.1f}M)")
        print("ç›®æ ‡å‚æ•°é‡: ~50M")
        print("=" * 60)
        
        return model
    
    def _create_dataloaders(self) -> Tuple[DataLoader, DataLoader, Dict]:
        """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
        train_loader, val_loader, vocab_info = create_dataloaders(
            data_dir=self.config.get('data_dir', 'data'),
            batch_size=self.config['batch_size'],
            max_seq_len=self.config['max_seq_len'],
            num_workers=self.config.get('num_workers', 0),
            test_size=0.1,
            add_rhythmic_info=True  # å¯ç”¨éŸµå¾‹ä¿¡æ¯
        )
        
        self.logger.info(f"æ•°æ®åŠ è½½å®Œæˆ:")
        self.logger.info(f"  è®­ç»ƒé›†: {len(train_loader)} æ‰¹æ¬¡")
        self.logger.info(f"  éªŒè¯é›†: {len(val_loader)} æ‰¹æ¬¡")
        self.logger.info(f"  è¯æ±‡è¡¨å¤§å°: {len(vocab_info['word2ix'])}")
        
        return train_loader, val_loader, vocab_info
    
    def _compute_rhythmic_positions(self, input_ids: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—éŸµå¾‹ä½ç½®ä¿¡æ¯ï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        # å¦‚æœbatchä¸­å·²ç»åŒ…å«char_positionsï¼Œç›´æ¥è¿”å›
        return None  # è¿™ä¸ªå‡½æ•°å°†è¢«ç§»é™¤ï¼Œç”±collate_fnç›´æ¥æä¾›
    
    def train_epoch(self) -> Dict[str, float]:
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        total_samples = 0
        
        pbar = tqdm(self.train_loader, desc=f"Epoch {self.current_epoch+1}")
        
        for batch_idx, batch in enumerate(pbar):
            # å‡†å¤‡è¾“å…¥æ•°æ®
            input_ids = batch['input_ids'].to(self.device)
            target_ids = batch['target_ids'].to(self.device)
            
            # ä½¿ç”¨DataLoaderé¢„è®¡ç®—çš„éŸµå¾‹ä½ç½®ï¼ˆæ ¸å¿ƒæœºåˆ¶1ï¼‰
            char_positions = batch['char_positions'].to(self.device)
            
            # å‰å‘ä¼ æ’­ï¼ˆä½¿ç”¨è½»é‡Transformer - æ ¸å¿ƒæœºåˆ¶3ï¼‰
            if self.scaler is not None:
                # æ··åˆç²¾åº¦è®­ç»ƒ
                with torch.cuda.amp.autocast():
                    logits = self.model(input_ids, char_positions=char_positions)
                    loss = self.criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))
                    loss = loss / self.config['accumulation_steps']
                
                # åå‘ä¼ æ’­
                self.scaler.scale(loss).backward()
            else:
                # æ ‡å‡†è®­ç»ƒ
                logits = self.model(input_ids, char_positions=char_positions)
                loss = self.criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))
                loss = loss / self.config['accumulation_steps']
                loss.backward()
            
            # æ¢¯åº¦ç´¯ç§¯
            if (batch_idx + 1) % self.config['accumulation_steps'] == 0:
                if self.scaler is not None:
                    # æ··åˆç²¾åº¦è®­ç»ƒçš„ä¼˜åŒ–æ­¥éª¤
                    self.scaler.unscale_(self.optimizer)
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    # æ ‡å‡†è®­ç»ƒçš„ä¼˜åŒ–æ­¥éª¤
                    torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
                    self.optimizer.step()
                
                self.scheduler.step()
                self.optimizer.zero_grad()
            
            # ç»Ÿè®¡
            total_loss += loss.item() * self.config['accumulation_steps']
            total_samples += input_ids.size(0)
            
            # æ›´æ–°è¿›åº¦æ¡
            pbar.set_postfix({
                'loss': f"{loss.item() * self.config['accumulation_steps']:.4f}",
                'lr': f"{self.scheduler.get_last_lr()[0]:.2e}"
            })
        
        avg_loss = total_loss / len(self.train_loader)
        return {'train_loss': avg_loss}
    
    def validate(self) -> Dict[str, float]:
        """éªŒè¯æ¨¡å‹"""
        self.model.eval()
        total_loss = 0
        total_samples = 0
        
        with torch.no_grad():
            for batch in tqdm(self.val_loader, desc="éªŒè¯ä¸­"):
                input_ids = batch['input_ids'].to(self.device)
                target_ids = batch['target_ids'].to(self.device)
                
                # ä½¿ç”¨DataLoaderé¢„è®¡ç®—çš„éŸµå¾‹ä½ç½®
                char_positions = batch['char_positions'].to(self.device)
                
                # å‰å‘ä¼ æ’­
                if self.scaler is not None:
                    with torch.cuda.amp.autocast():
                        logits = self.model(input_ids, char_positions=char_positions)
                        loss = self.criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))
                else:
                    logits = self.model(input_ids, char_positions=char_positions)
                    loss = self.criterion(logits.view(-1, logits.size(-1)), target_ids.view(-1))
                
                total_loss += loss.item()
                total_samples += input_ids.size(0)
        
        avg_loss = total_loss / len(self.val_loader)
        perplexity = torch.exp(torch.tensor(avg_loss)).item()
        
        return {
            'val_loss': avg_loss,
            'perplexity': perplexity
        }
    
    def generate_samples(self, num_samples: int = 3) -> List[str]:
        """ç”Ÿæˆæ ·æœ¬è¯—æ­Œï¼ˆé›†æˆçº¦æŸè§£ç ç®—æ³• - æ ¸å¿ƒæœºåˆ¶2ï¼‰"""
        samples = []
        prompts = ["æ˜¥", "æœˆ", "å±±"]
        poem_types = ["ä¸ƒè¨€ç»å¥", "äº”è¨€ç»å¥", "ä¸ƒè¨€å¾‹è¯—"]
        
        # åˆ›å»ºçº¦æŸè§£ç ç”Ÿæˆå™¨
        from utils.generate_utils import ConstrainedPoetryGenerator
        constrained_generator = ConstrainedPoetryGenerator(
            self.model, self.vocab_info['ix2word'], self.vocab_info['word2ix']
        )
        
        for i, prompt in enumerate(prompts[:num_samples]):
            poem_type = poem_types[i % len(poem_types)]
            try:
                # ä½¿ç”¨çº¦æŸè§£ç ç”Ÿæˆä¸¥æ ¼æ ¼å¾‹è¯—æ­Œ
                poem = constrained_generator.generate_constrained_poem(
                    prompt=prompt,
                    poem_type=poem_type,
                    temperature=0.8,
                    top_k=50,
                    top_p=0.9
                )
                samples.append(f"ã€{poem_type}ã€‘æç¤º'{prompt}':\n{poem}")
            except Exception as e:
                samples.append(f"ã€{poem_type}ã€‘æç¤º'{prompt}': [ç”Ÿæˆå¤±è´¥: {e}]")
        
        return samples
    
    def train(self, num_epochs: int):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self.logger.info(f"å¼€å§‹è®­ç»ƒ - ç›®æ ‡epochs: {num_epochs}")
        self.logger.info(f"ä¸‰å¤§æ ¸å¿ƒæœºåˆ¶: âœ“éŸµå¾‹ä½ç½®ç¼–ç  âœ“å¼ºåˆ¶å¥é•¿è§£ç  âœ“è½»é‡Transformer")
        
        for epoch in range(self.current_epoch, num_epochs):
            self.current_epoch = epoch
            
            # è®­ç»ƒ
            train_metrics = self.train_epoch()
            
            # éªŒè¯
            val_metrics = self.validate()
            
            # è®°å½•å­¦ä¹ ç‡
            current_lr = self.scheduler.get_last_lr()[0]
            
            # åˆå¹¶æŒ‡æ ‡
            metrics = {**train_metrics, **val_metrics, 'learning_rate': current_lr}
            self.training_stats.append(metrics)
            
            # æ›´æ–°æŒ‡æ ‡å†å²
            self.metrics_history['epochs'].append(epoch + 1)
            self.metrics_history['train_loss'].append(metrics['train_loss'])
            self.metrics_history['val_loss'].append(metrics['val_loss'])
            self.metrics_history['learning_rate'].append(current_lr)
            
            # æ—¥å¿—è®°å½•
            self.logger.info(
                f"Epoch {epoch+1}/{num_epochs} - "
                f"Train Loss: {metrics['train_loss']:.4f} - "
                f"Val Loss: {metrics['val_loss']:.4f} - "
                f"Perplexity: {metrics['perplexity']:.2f} - "
                f"LR: {current_lr:.2e}"
            )
            
            # ç”Ÿæˆå¯è§†åŒ–ï¼ˆæ¯5ä¸ªepochï¼‰
            if (epoch + 1) % 5 == 0:
                try:
                    self.visualizer.plot_training_curves(
                        self.metrics_history,
                        self.metrics_history['epochs'],
                        title=f"Transformer Poetry Training (Epoch {epoch+1})",
                        save_name=f"training_curves_epoch_{epoch+1:03d}.png"
                    )
                except Exception as e:
                    self.logger.warning(f"å¯è§†åŒ–ç”Ÿæˆå¤±è´¥: {e}")
            
            # ç”Ÿæˆæ ·æœ¬
            if (epoch + 1) % 5 == 0:
                samples = self.generate_samples()
                self.logger.info("ç”Ÿæˆæ ·æœ¬:")
                for sample in samples:
                    self.logger.info(f"  {sample}")
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            is_best = metrics['val_loss'] < self.best_loss
            if is_best:
                self.best_loss = metrics['val_loss']
            
            self._save_checkpoint(epoch, metrics, is_best)
            
            # æ—©åœæ£€æŸ¥
            if self._should_early_stop():
                self.logger.info("è§¦å‘æ—©åœï¼Œè®­ç»ƒç»“æŸ")
                break
        
        # è®­ç»ƒç»“æŸåç”Ÿæˆæœ€ç»ˆå›¾è¡¨
        try:
            self.logger.info("ç”Ÿæˆæœ€ç»ˆè®­ç»ƒæ›²çº¿...")
            final_plot_path = self.visualizer.plot_training_curves(
                self.metrics_history,
                self.metrics_history['epochs'],
                title="Final Transformer Poetry Training Results",
                save_name="final_training_curves.png"
            )
            
            # ç”Ÿæˆå•ç‹¬çš„æŸå¤±å¯¹æ¯”å›¾
            self.visualizer.plot_loss_comparison(
                self.metrics_history['train_loss'],
                self.metrics_history['val_loss'],
                self.metrics_history['epochs'],
                save_name="final_loss_comparison.png"
            )
            
            # ç”Ÿæˆå­¦ä¹ ç‡å›¾
            if len(self.metrics_history['learning_rate']) > 1:
                self.visualizer.plot_learning_rate_schedule(
                    self.metrics_history['learning_rate'],
                    self.metrics_history['epochs'],
                    save_name="final_learning_rate_schedule.png"
                )
            
            # ç”Ÿæˆå›°æƒ‘åº¦å›¾
            train_perplexity = [np.exp(loss) for loss in self.metrics_history['train_loss']]
            val_perplexity = [np.exp(loss) for loss in self.metrics_history['val_loss']]
            self.visualizer.plot_perplexity_trend(
                train_perplexity,
                val_perplexity,
                self.metrics_history['epochs'],
                save_name="final_perplexity_trend.png"
            )
            
            self.logger.info(f"ğŸ“Š æ‰€æœ‰è®­ç»ƒå›¾è¡¨å·²ä¿å­˜åˆ° plots/ ç›®å½•")
            
        except Exception as e:
            self.logger.error(f"æœ€ç»ˆå›¾è¡¨ç”Ÿæˆå¤±è´¥: {e}")
        
        self.logger.info("è®­ç»ƒå®Œæˆï¼")
    
    def _save_checkpoint(self, epoch: int, metrics: Dict, is_best: bool = False):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'metrics': metrics,
            'config': self.config,
            'vocab_info': self.vocab_info,
            'training_stats': self.training_stats
        }
        
        # ä¿å­˜æœ€æ–°æ£€æŸ¥ç‚¹
        save_checkpoint(checkpoint, is_best, 
                       checkpoint_dir=f"checkpoints/{self.environment}")
        
        if is_best:
            self.logger.info(f"ğŸ‰ æ–°çš„æœ€ä½³æ¨¡å‹! Val Loss: {metrics['val_loss']:.4f}")
    
    def _resume_training(self, checkpoint_path: str):
        """æ¢å¤è®­ç»ƒ"""
        checkpoint = load_checkpoint(checkpoint_path)
        
        self.model.load_state_dict(checkpoint['model_state_dict'])
        self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
        self.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
        
        self.current_epoch = checkpoint['epoch'] + 1
        self.training_stats = checkpoint.get('training_stats', [])
        self.best_loss = min([stat['val_loss'] for stat in self.training_stats], 
                           default=float('inf'))
        
        # æ¢å¤æŒ‡æ ‡å†å²
        if self.training_stats:
            self.metrics_history = {
                'epochs': list(range(1, len(self.training_stats) + 1)),
                'train_loss': [stat['train_loss'] for stat in self.training_stats],
                'val_loss': [stat['val_loss'] for stat in self.training_stats],
                'learning_rate': [stat.get('learning_rate', 0) for stat in self.training_stats]
            }
        
        self.logger.info(f"ä»epoch {self.current_epoch} æ¢å¤è®­ç»ƒï¼Œå·²æ¢å¤ {len(self.training_stats)} ä¸ªepochçš„æŒ‡æ ‡")
    
    def _should_early_stop(self, patience: int = None) -> bool:
        """æ—©åœæ£€æŸ¥ - ä¿®å¤é€»è¾‘é”™è¯¯"""
        if patience is None:
            patience = config.EARLY_STOPPING_PATIENCE
            
        if len(self.training_stats) < patience:
            return False
        
        # æ£€æŸ¥æœ€è¿‘patienceä¸ªepochä¸­éªŒè¯æŸå¤±æ˜¯å¦æ²¡æœ‰æ”¹å–„
        recent_losses = [stat['val_loss'] for stat in self.training_stats[-patience:]]
        min_recent_loss = min(recent_losses)
        
        # å¦‚æœæœ€è¿‘patienceä¸ªepochä¸­æœ€å°æŸå¤±éƒ½æ²¡æœ‰ä½äºå†å²æœ€ä½³ï¼Œåˆ™æ—©åœ
        # ä½†å¦‚æœåˆšå¥½ç­‰äºæœ€ä½³ï¼ˆå³æœ€è¿‘åˆšè¾¾åˆ°æœ€ä½³ï¼‰ï¼Œä¸æ—©åœ
        return min_recent_loss > self.best_loss


def main():
    """ä¸»å‡½æ•°"""
    # ç”¨æˆ·é…ç½®ï¼ˆè¦†ç›–é»˜è®¤è®¾ç½®ï¼‰
    user_config = {
        'data_dir': 'data',
        'pad_token_id': 0,
        'num_epochs': config.NUM_EPOCHS,  # ä½¿ç”¨config.pyä¸­çš„epoché…ç½®
        'save_every': 5,
        'validate_every': 1,
        'warmup_steps': 1000,
        'max_grad_norm': 1.0,
        'weight_decay': 0.01,
    }
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    os.makedirs('checkpoints', exist_ok=True)
    os.makedirs('logs', exist_ok=True)
    
    # è®­ç»ƒå™¨
    trainer = PoetryTransformerTrainer(user_config)
    
    # å¼€å§‹è®­ç»ƒ
    trainer.train(user_config['num_epochs'])


if __name__ == "__main__":
    main() 