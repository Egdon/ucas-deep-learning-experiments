#!/usr/bin/env python3
"""
Dataset classes and data processing utilities for poetry generation.
Includes standard poetry dataset and acrostic poetry data augmentation.
"""

import os
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from collections import Counter
import random
from typing import List, Tuple, Dict, Optional

from .config import config

class PoetryDataset(Dataset):
    """Standard poetry dataset for sequence-to-sequence training."""
    
    def __init__(self, data_path: str = None, mode: str = 'train'):
        """
        Initialize poetry dataset.
        
        Args:
            data_path: Path to NPZ data file or directory containing tang.npz
            mode: 'train', 'val', or 'test'
        """
        if data_path is None:
            self.data_path = config.DATA_PATH
        else:
            # å¦‚æœä¼ å…¥çš„æ˜¯ç›®å½•ï¼Œåˆ™æŸ¥æ‰¾tang.npzæ–‡ä»¶
            if os.path.isdir(data_path):
                self.data_path = os.path.join(data_path, 'tang.npz')
            else:
                self.data_path = data_path
        
        self.mode = mode
        
        self.data, self.ix2word, self.word2ix = self.load_data()
        self.vocab_size = len(self.word2ix)
        
        # é¢„å¤„ç†æ•°æ®
        self.processed_data = self.preprocess_data()
        
        # åˆ’åˆ†æ•°æ®é›†
        self.train_data, self.val_data, self.test_data = self.split_data()
        
        # æ ¹æ®æ¨¡å¼é€‰æ‹©æ•°æ®
        if mode == 'train':
            self.current_data = self.train_data
        elif mode == 'val':
            self.current_data = self.val_data
        else:
            self.current_data = self.test_data
    
    def load_data(self) -> Tuple[np.ndarray, Dict, Dict]:
        """åŠ è½½NPZæ•°æ®æ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(self.data_path):
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•æŸ¥æ‰¾å¯èƒ½çš„æ–‡ä»¶
                possible_paths = [
                    'data/tang.npz',
                    'tang.npz',
                    os.path.join(os.path.dirname(self.data_path), 'tang.npz')
                ]
                
                found_path = None
                for path in possible_paths:
                    if os.path.exists(path):
                        found_path = path
                        break
                
                if found_path:
                    print(f"ğŸ” æ•°æ®æ–‡ä»¶æœªåœ¨ {self.data_path} æ‰¾åˆ°ï¼Œä½¿ç”¨: {found_path}")
                    self.data_path = found_path
                else:
                    raise FileNotFoundError(
                        f"æ•°æ®æ–‡ä»¶æœªæ‰¾åˆ°: {self.data_path}\n"
                        f"è¯·ç¡®ä¿ä»¥ä¸‹ä»»ä¸€æ–‡ä»¶å­˜åœ¨:\n"
                        f"  - data/tang.npz\n"
                        f"  - tang.npz\n"
                        f"å½“å‰å·¥ä½œç›®å½•: {os.getcwd()}"
                    )
            
            print(f"ğŸ“ åŠ è½½æ•°æ®æ–‡ä»¶: {self.data_path}")
            dataset = np.load(self.data_path, allow_pickle=True)
            data = dataset['data']
            ix2word = dataset['ix2word'].item()
            word2ix = dataset['word2ix'].item()
            print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: {len(data)} é¦–è¯—ï¼Œè¯æ±‡é‡ {len(word2ix)}")
            return data, ix2word, word2ix
        except Exception as e:
            raise RuntimeError(f"Failed to load dataset from {self.data_path}: {e}")
    
    def preprocess_data(self) -> List[np.ndarray]:
        """é¢„å¤„ç†æ•°æ®ï¼Œæå–æœ‰æ•ˆåºåˆ—"""
        processed = []
        padding_token_id = self.word2ix.get(config.PADDING_TOKEN, None)
        
        for poem in self.data:
            if padding_token_id is not None:
                # æ‰¾åˆ°éå¡«å……éƒ¨åˆ†
                non_padding_positions = np.where(poem != padding_token_id)[0]
                if len(non_padding_positions) > 10:  # æœ€å°‘10ä¸ªå­—ç¬¦
                    valid_sequence = poem[non_padding_positions]
                    processed.append(valid_sequence)
            else:
                processed.append(poem)
        
        return processed
    
    def split_data(self, train_ratio: float = 0.8, val_ratio: float = 0.1) -> Tuple[List, List, List]:
        """åˆ’åˆ†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†"""
        total_size = len(self.processed_data)
        train_size = int(total_size * train_ratio)
        val_size = int(total_size * val_ratio)
        
        # éšæœºæ‰“ä¹±æ•°æ®
        indices = list(range(total_size))
        random.shuffle(indices)
        
        train_indices = indices[:train_size]
        val_indices = indices[train_size:train_size + val_size]
        test_indices = indices[train_size + val_size:]
        
        train_data = [self.processed_data[i] for i in train_indices]
        val_data = [self.processed_data[i] for i in val_indices]
        test_data = [self.processed_data[i] for i in test_indices]
        
        return train_data, val_data, test_data
    
    def __len__(self) -> int:
        return len(self.current_data)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è·å–å•ä¸ªæ ·æœ¬
        
        Returns:
            input_seq: è¾“å…¥åºåˆ—
            target_seq: ç›®æ ‡åºåˆ—  
            mode_flag: æ¨¡å¼æ ‡å¿—ï¼ˆ0=ç»­å†™ï¼Œ1=è—å¤´è¯—ï¼‰
        """
        poem = self.current_data[idx]
        
        # ç¡®ä¿åºåˆ—é•¿åº¦
        max_len = min(len(poem), config.SEQUENCE_LENGTH)
        if max_len < 2:
            # å¤„ç†è¿‡çŸ­åºåˆ—
            poem = np.pad(poem, (0, 2 - len(poem)), constant_values=self.word2ix[config.PADDING_TOKEN])
            max_len = 2
        
        # åˆ›å»ºè¾“å…¥å’Œç›®æ ‡åºåˆ—
        input_seq = poem[:max_len-1]
        target_seq = poem[1:max_len]
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        if len(input_seq) != len(target_seq):
            min_len = min(len(input_seq), len(target_seq))
            input_seq = input_seq[:min_len]
            target_seq = target_seq[:min_len]
        
        # è½¬æ¢ä¸ºtensor
        input_tensor = torch.LongTensor(input_seq)
        target_tensor = torch.LongTensor(target_seq)
        mode_tensor = torch.LongTensor([0])  # 0è¡¨ç¤ºæ ‡å‡†ç»­å†™æ¨¡å¼
        
        return input_tensor, target_tensor, mode_tensor

class AcrosticDataset(Dataset):
    """è—å¤´è¯—æ•°æ®é›†ï¼Œç”¨äºè®­ç»ƒè—å¤´è¯—ç”ŸæˆåŠŸèƒ½"""
    
    def __init__(self, base_dataset: PoetryDataset, num_acrostic_samples: int = 5000):
        """
        åŸºäºåŸºç¡€æ•°æ®é›†åˆ›å»ºè—å¤´è¯—è®­ç»ƒæ•°æ®
        
        Args:
            base_dataset: åŸºç¡€è¯—è¯æ•°æ®é›†
            num_acrostic_samples: ç”Ÿæˆçš„è—å¤´è¯—æ ·æœ¬æ•°é‡
        """
        self.base_dataset = base_dataset
        self.ix2word = base_dataset.ix2word
        self.word2ix = base_dataset.word2ix
        
        # ç”Ÿæˆè—å¤´è¯—è®­ç»ƒæ•°æ®
        self.acrostic_data = self.generate_acrostic_data(num_acrostic_samples)
    
    def generate_acrostic_data(self, num_samples: int) -> List[Tuple[List[int], List[int]]]:
        """
        ä»åŸºç¡€æ•°æ®é›†ç”Ÿæˆè—å¤´è¯—è®­ç»ƒæ ·æœ¬
        
        Returns:
            List of (acrostic_chars, poem_sequence) pairs
        """
        acrostic_samples = []
        
        for _ in range(num_samples):
            # éšæœºé€‰æ‹©ä¸€é¦–è¯—
            poem_idx = random.randint(0, len(self.base_dataset.current_data) - 1)
            poem = self.base_dataset.current_data[poem_idx]
            
            # è½¬æ¢ä¸ºæ–‡æœ¬
            poem_text = self.indices_to_text(poem)
            sentences = poem_text.split('ã€‚')
            
            # æå–å¥é¦–å­—ç¬¦ä½œä¸ºè—å¤´
            first_chars = []
            valid_sentences = []
            
            for sentence in sentences:
                sentence = sentence.strip().replace('<START>', '').replace('<EOP>', '')
                if len(sentence) > 0:
                    first_chars.append(sentence[0])
                    valid_sentences.append(sentence)
            
            # ç¡®ä¿è‡³å°‘æœ‰2ä¸ªå¥å­
            if len(first_chars) >= 2:
                # æ„é€ è®­ç»ƒæ ·æœ¬
                acrostic_indices = [self.word2ix.get(char, self.word2ix[config.PADDING_TOKEN]) 
                                  for char in first_chars]
                
                acrostic_samples.append((acrostic_indices, poem))
        
        return acrostic_samples
    
    def indices_to_text(self, indices: np.ndarray) -> str:
        """å°†ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬"""
        text = ""
        for idx in indices:
            if idx in self.ix2word:
                char = self.ix2word[idx]
                if char == config.PADDING_TOKEN:
                    break
                text += char
        return text
    
    def __len__(self) -> int:
        return len(self.acrostic_data)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        è·å–è—å¤´è¯—è®­ç»ƒæ ·æœ¬
        
        Returns:
            input_seq: è¾“å…¥åºåˆ—ï¼ˆåŒ…å«è—å¤´çº¦æŸï¼‰
            target_seq: ç›®æ ‡åºåˆ—
            mode_flag: æ¨¡å¼æ ‡å¿—ï¼ˆ1=è—å¤´è¯—ï¼‰
        """
        acrostic_chars, poem = self.acrostic_data[idx]
        
        # åˆ›å»ºå¸¦çº¦æŸçš„è¾“å…¥åºåˆ—
        max_len = min(len(poem), config.SEQUENCE_LENGTH)
        if max_len < 2:
            poem = np.pad(poem, (0, 2 - len(poem)), constant_values=self.word2ix[config.PADDING_TOKEN])
            max_len = 2
        
        input_seq = poem[:max_len-1]
        target_seq = poem[1:max_len]
        
        # ç¡®ä¿é•¿åº¦ä¸€è‡´
        if len(input_seq) != len(target_seq):
            min_len = min(len(input_seq), len(target_seq))
            input_seq = input_seq[:min_len]
            target_seq = target_seq[:min_len]
        
        input_tensor = torch.LongTensor(input_seq)
        target_tensor = torch.LongTensor(target_seq)
        mode_tensor = torch.LongTensor([1])  # 1è¡¨ç¤ºè—å¤´è¯—æ¨¡å¼
        
        return input_tensor, target_tensor, mode_tensor

class MixedDataset(Dataset):
    """æ··åˆæ•°æ®é›†ï¼Œç»“åˆæ ‡å‡†è¯—è¯å’Œè—å¤´è¯—è®­ç»ƒæ•°æ®"""
    
    def __init__(self, poetry_dataset: PoetryDataset, acrostic_dataset: AcrosticDataset, 
                 mix_ratio: float = 0.7):
        """
        åˆ›å»ºæ··åˆæ•°æ®é›†
        
        Args:
            poetry_dataset: æ ‡å‡†è¯—è¯æ•°æ®é›†
            acrostic_dataset: è—å¤´è¯—æ•°æ®é›†
            mix_ratio: æ ‡å‡†è¯—è¯æ•°æ®çš„æ¯”ä¾‹
        """
        self.poetry_dataset = poetry_dataset
        self.acrostic_dataset = acrostic_dataset
        self.mix_ratio = mix_ratio
        
        # è®¡ç®—é‡‡æ ·æ•°é‡
        total_poetry = len(poetry_dataset)
        total_acrostic = len(acrostic_dataset)
        
        self.poetry_samples = int(total_poetry * mix_ratio)
        self.acrostic_samples = int(total_poetry * (1 - mix_ratio))
        
        # ç¡®ä¿ä¸è¶…è¿‡å¯ç”¨æ•°æ®
        self.poetry_samples = min(self.poetry_samples, total_poetry)
        self.acrostic_samples = min(self.acrostic_samples, total_acrostic)
        
        self.total_samples = self.poetry_samples + self.acrostic_samples
    
    def __len__(self) -> int:
        return self.total_samples
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """è·å–æ··åˆæ ·æœ¬"""
        if idx < self.poetry_samples:
            # æ ‡å‡†è¯—è¯æ ·æœ¬
            poetry_idx = idx % len(self.poetry_dataset)
            return self.poetry_dataset[poetry_idx]
        else:
            # è—å¤´è¯—æ ·æœ¬
            acrostic_idx = (idx - self.poetry_samples) % len(self.acrostic_dataset)
            return self.acrostic_dataset[acrostic_idx]

def create_dataloaders(data_dir: str = None, 
                      batch_size: int = None,
                      max_seq_len: int = None,
                      test_size: float = 0.1,
                      num_workers: int = None,
                      include_acrostic: bool = True,
                      add_rhythmic_info: bool = False) -> Tuple[DataLoader, DataLoader, Dict]:
    """
    åˆ›å»ºè®­ç»ƒã€éªŒè¯æ•°æ®åŠ è½½å™¨ï¼Œæ”¯æŒéŸµå¾‹ä¿¡æ¯
    
    Args:
        data_dir: æ•°æ®ç›®å½•è·¯å¾„
        batch_size: æ‰¹æ¬¡å¤§å°
        max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        test_size: éªŒè¯é›†æ¯”ä¾‹
        num_workers: å·¥ä½œè¿›ç¨‹æ•°
        include_acrostic: æ˜¯å¦åŒ…å«è—å¤´è¯—æ•°æ®
        add_rhythmic_info: æ˜¯å¦æ·»åŠ éŸµå¾‹ä¿¡æ¯
        
    Returns:
        train_loader, val_loader, vocab_info
    """
    batch_size = batch_size or config.BATCH_SIZE
    max_seq_len = max_seq_len or config.SEQUENCE_LENGTH
    num_workers = num_workers or config.NUM_WORKERS
    
    # åˆ›å»ºåŸºç¡€æ•°æ®é›†
    train_dataset = PoetryDataset(data_dir, mode='train')
    val_dataset = PoetryDataset(data_dir, mode='val')
    
    if include_acrostic:
        # åˆ›å»ºè—å¤´è¯—æ•°æ®é›†
        acrostic_train = AcrosticDataset(train_dataset, num_acrostic_samples=3000)
        acrostic_val = AcrosticDataset(val_dataset, num_acrostic_samples=500)
        
        # åˆ›å»ºæ··åˆæ•°æ®é›†
        train_dataset = MixedDataset(train_dataset, acrostic_train)
        val_dataset = MixedDataset(val_dataset, acrostic_val)
    
    # è·å–è¯æ±‡ä¿¡æ¯
    if include_acrostic:
        word2ix = train_dataset.poetry_dataset.word2ix
        ix2word = train_dataset.poetry_dataset.ix2word
    else:
        word2ix = train_dataset.word2ix
        ix2word = train_dataset.ix2word
    
    vocab_info = {
        'word2ix': word2ix,
        'ix2word': ix2word,
        'vocab_size': len(word2ix)
    }
    
    # è‡ªå®šä¹‰collateå‡½æ•°å¤„ç†å˜é•¿åºåˆ—å’ŒéŸµå¾‹ä¿¡æ¯
    def collate_fn(batch):
        input_seqs, target_seqs, mode_flags = zip(*batch)
        
        # å¡«å……åˆ°æŒ‡å®šé•¿åº¦
        actual_max_len = min(max_seq_len, max(len(seq) for seq in input_seqs))
        padding_id = word2ix[config.PADDING_TOKEN]
        
        padded_inputs = []
        padded_targets = []
        
        for input_seq, target_seq in zip(input_seqs, target_seqs):
            # æˆªæ–­æˆ–å¡«å……åˆ°ç›®æ ‡é•¿åº¦
            if len(input_seq) > actual_max_len:
                input_seq = input_seq[:actual_max_len]
                target_seq = target_seq[:actual_max_len]
            
            # å¡«å……è¾“å…¥åºåˆ—
            padded_input = torch.cat([
                input_seq,
                torch.full((actual_max_len - len(input_seq),), padding_id, dtype=torch.long)
            ])
            # å¡«å……ç›®æ ‡åºåˆ—
            padded_target = torch.cat([
                target_seq,
                torch.full((actual_max_len - len(target_seq),), padding_id, dtype=torch.long)
            ])
            
            padded_inputs.append(padded_input)
            padded_targets.append(padded_target)
        
        batch_dict = {
            'input_ids': torch.stack(padded_inputs),
            'target_ids': torch.stack(padded_targets),
            'mode_flags': torch.stack(mode_flags)
        }
        
        # å¦‚æœéœ€è¦éŸµå¾‹ä¿¡æ¯ï¼Œè®¡ç®—å­—ç¬¦ä½ç½®ï¼ˆå‘é‡åŒ–ç‰ˆæœ¬ï¼‰
        if add_rhythmic_info:
            char_positions = []
            period_id = word2ix.get('ã€‚', -1)
            
            for input_seq in batch_dict['input_ids']:
                # å‘é‡åŒ–è®¡ç®—éŸµå¾‹ä½ç½®
                pos_seq = torch.ones_like(input_seq, dtype=torch.long)
                
                # æ‰¾åˆ°å¥å·ä½ç½®
                period_mask = (input_seq == period_id)
                period_indices = period_mask.nonzero(as_tuple=True)[0]
                
                # è®¡ç®—æ¯ä¸ªä½ç½®åœ¨å¥å­ä¸­çš„ä½ç½®
                if len(period_indices) > 0:
                    # ä¸ºæ¯ä¸ªå¥å­æ®µåˆ†åˆ«è®¡ç®—ä½ç½®
                    start_idx = 0
                    for period_idx in period_indices:
                        segment_len = period_idx - start_idx + 1
                        pos_seq[start_idx:period_idx+1] = torch.arange(1, segment_len + 1, dtype=torch.long)
                        start_idx = period_idx + 1
                    
                    # å¤„ç†æœ€åä¸€ä¸ªå¥å­æ®µ
                    if start_idx < len(input_seq):
                        remaining_len = len(input_seq) - start_idx
                        pos_seq[start_idx:] = torch.arange(1, remaining_len + 1, dtype=torch.long)
                else:
                    # æ²¡æœ‰å¥å·ï¼Œæ•´ä¸ªåºåˆ—æ˜¯ä¸€ä¸ªå¥å­
                    pos_seq = torch.arange(1, len(input_seq) + 1, dtype=torch.long)
                
                # é™åˆ¶ä½ç½®åœ¨1-7èŒƒå›´å†…
                pos_seq = torch.clamp(pos_seq, min=1, max=7)
                char_positions.append(pos_seq)
            
            batch_dict['char_positions'] = torch.stack(char_positions)
        
        return batch_dict
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=torch.cuda.is_available()
    )
    
    val_loader = DataLoader(
        val_dataset,
        batch_size=batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=torch.cuda.is_available()
    )
    
    return train_loader, val_loader, vocab_info

if __name__ == "__main__":
    # æµ‹è¯•æ•°æ®é›†
    print("Testing poetry dataset...")
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloaders = create_dataloaders()
    
    # æµ‹è¯•è®­ç»ƒæ•°æ®åŠ è½½å™¨
    train_loader = dataloaders['train']
    print(f"Train dataset size: {len(train_loader.dataset)}")
    
    # è·å–ä¸€ä¸ªæ‰¹æ¬¡
    for batch_idx, (inputs, targets, modes) in enumerate(train_loader):
        print(f"Batch {batch_idx}:")
        print(f"  Input shape: {inputs.shape}")
        print(f"  Target shape: {targets.shape}")
        print(f"  Mode shape: {modes.shape}")
        print(f"  Mode values: {modes.flatten()[:10].tolist()}")
        
        if batch_idx >= 2:  # åªæµ‹è¯•å‰3ä¸ªæ‰¹æ¬¡
            break
    
    print("Dataset testing completed!") 