#!/usr/bin/env python3
"""
Generation utilities for poetry generation.
Includes sampling strategies, text processing and quality assessment.
"""

import torch
import torch.nn.functional as F
import numpy as np
import re
from typing import List, Dict, Tuple, Optional, Union
import random
from collections import Counter

from models import config

class TextSampler:
    """æ–‡æœ¬é‡‡æ ·ç­–ç•¥é›†åˆ"""
    
    @staticmethod
    def greedy_sample(logits: torch.Tensor) -> torch.Tensor:
        """è´ªå©ªé‡‡æ · - é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„token"""
        return torch.argmax(logits, dim=-1)
    
    @staticmethod
    def random_sample(logits: torch.Tensor, temperature: float = 1.0) -> torch.Tensor:
        """éšæœºé‡‡æ ·"""
        if temperature <= 0:
            return TextSampler.greedy_sample(logits)
        
        # åº”ç”¨æ¸©åº¦
        scaled_logits = logits / temperature
        probs = F.softmax(scaled_logits, dim=-1)
        
        return torch.multinomial(probs, num_samples=1).squeeze(-1)
    
    @staticmethod
    def top_k_sample(logits: torch.Tensor, k: int, temperature: float = 1.0) -> torch.Tensor:
        """Top-Ké‡‡æ ·"""
        if k <= 0:
            return TextSampler.random_sample(logits, temperature)
        
        # è·å–top-k logits
        top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)
        
        # åº”ç”¨æ¸©åº¦
        scaled_logits = top_k_logits / temperature
        probs = F.softmax(scaled_logits, dim=-1)
        
        # é‡‡æ ·
        sampled_indices = torch.multinomial(probs, num_samples=1).squeeze(-1)
        
        # æ˜ å°„å›åŸå§‹è¯æ±‡è¡¨
        return top_k_indices.gather(-1, sampled_indices.unsqueeze(-1)).squeeze(-1)
    
    @staticmethod
    def nucleus_sample(logits: torch.Tensor, p: float, temperature: float = 1.0) -> torch.Tensor:
        """Nucleus (Top-P) é‡‡æ ·"""
        if p <= 0 or p >= 1:
            return TextSampler.random_sample(logits, temperature)
        
        # åº”ç”¨æ¸©åº¦
        scaled_logits = logits / temperature
        probs = F.softmax(scaled_logits, dim=-1)
        
        # æ’åº
        sorted_probs, sorted_indices = torch.sort(probs, descending=True, dim=-1)
        
        # è®¡ç®—ç´¯ç§¯æ¦‚ç‡
        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
        
        # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡pçš„ä½ç½®
        sorted_indices_to_remove = cumulative_probs > p
        
        # ä¿ç•™ç¬¬ä¸€ä¸ªè¶…è¿‡é˜ˆå€¼çš„token
        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
        sorted_indices_to_remove[..., 0] = 0
        
        # å°†è¦ç§»é™¤çš„tokenæ¦‚ç‡è®¾ä¸º0
        sorted_probs[sorted_indices_to_remove] = 0
        
        # é‡æ–°å½’ä¸€åŒ–
        sorted_probs = sorted_probs / sorted_probs.sum(dim=-1, keepdim=True)
        
        # é‡‡æ ·
        sampled_indices = torch.multinomial(sorted_probs, num_samples=1).squeeze(-1)
        
        # æ˜ å°„å›åŸå§‹ç´¢å¼•
        return sorted_indices.gather(-1, sampled_indices.unsqueeze(-1)).squeeze(-1)
    
    @staticmethod
    def combined_sample(logits: torch.Tensor, top_k: int = 50, top_p: float = 0.9,
                       temperature: float = 0.8) -> torch.Tensor:
        """ç»„åˆé‡‡æ ·ç­–ç•¥ï¼šTop-K + Top-P"""
        # å…ˆåº”ç”¨Top-K
        if top_k > 0 and top_k < logits.size(-1):
            top_k_logits, top_k_indices = torch.topk(logits, top_k, dim=-1)
            filtered_logits = torch.full_like(logits, float('-inf'))
            filtered_logits.scatter_(-1, top_k_indices, top_k_logits)
        else:
            filtered_logits = logits
        
        # å†åº”ç”¨Top-P
        return TextSampler.nucleus_sample(filtered_logits, top_p, temperature)

class TextProcessor:
    """æ–‡æœ¬å¤„ç†å’Œæ ¼å¼åŒ–å·¥å…·"""
    
    def __init__(self, ix2word: Dict[int, str], word2ix: Dict[str, int]):
        self.ix2word = ix2word
        self.word2ix = word2ix
        self.special_tokens = {
            config.START_TOKEN, config.END_TOKEN, config.PADDING_TOKEN,
            config.ACROSTIC_MODE_TOKEN, config.CONTINUE_MODE_TOKEN
        }
    
    def indices_to_text(self, indices: Union[torch.Tensor, List[int]], 
                       remove_special: bool = True) -> str:
        """å°†ç´¢å¼•åºåˆ—è½¬æ¢ä¸ºæ–‡æœ¬"""
        if isinstance(indices, torch.Tensor):
            indices = indices.cpu().numpy().tolist()
        
        text = ""
        for idx in indices:
            if idx in self.ix2word:
                char = self.ix2word[idx]
                if char == config.PADDING_TOKEN:
                    break  # é‡åˆ°å¡«å……ç¬¦åœæ­¢
                elif remove_special and char in self.special_tokens:
                    continue  # è·³è¿‡ç‰¹æ®Šæ ‡è®°
                else:
                    text += char
        
        return text
    
    def text_to_indices(self, text: str, add_special: bool = False) -> List[int]:
        """å°†æ–‡æœ¬è½¬æ¢ä¸ºç´¢å¼•åºåˆ—"""
        indices = []
        
        if add_special:
            start_idx = self.word2ix.get(config.START_TOKEN)
            if start_idx is not None:
                indices.append(start_idx)
        
        for char in text:
            if char in self.word2ix:
                indices.append(self.word2ix[char])
            # è·³è¿‡æœªçŸ¥å­—ç¬¦
        
        if add_special:
            end_idx = self.word2ix.get(config.END_TOKEN)
            if end_idx is not None:
                indices.append(end_idx)
        
        return indices
    
    def format_poem(self, text: str, poem_type: str = "ç»­å†™") -> str:
        """æ ¼å¼åŒ–è¯—æ­Œè¾“å‡º"""
        # åŸºç¡€æ¸…ç†
        text = self.clean_text(text)
        
        # æŒ‰å¥å·åˆ†å‰²
        sentences = [s.strip() for s in text.split('ã€‚') if s.strip()]
        
        if not sentences:
            return text
        
        # æ ¼å¼åŒ–è¾“å‡º
        formatted = f"ã€{poem_type}è¯—ã€‘\n"
        formatted += "=" * 30 + "\n"
        
        for i, sentence in enumerate(sentences, 1):
            if sentence:
                formatted += f"{sentence}ã€‚\n"
        
        formatted += "=" * 30
        return formatted
    
    def clean_text(self, text: str) -> str:
        """æ¸…ç†ç”Ÿæˆçš„æ–‡æœ¬"""
        # ç§»é™¤è¿ç»­çš„é‡å¤å­—ç¬¦
        text = re.sub(r'(.)\1{2,}', r'\1\1', text)
        
        # ç§»é™¤å¼‚å¸¸çš„æ ‡ç‚¹ç»„åˆ
        text = re.sub(r'[ã€‚ï¼Œã€ï¼›ï¼šï¼ï¼Ÿ]{2,}', 'ã€‚', text)
        
        # ç¡®ä¿å¥å­ä»¥å¥å·ç»“å°¾
        if text and not text.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
            text += 'ã€‚'
        
        return text
    
    def extract_sentences(self, text: str) -> List[str]:
        """æå–å¥å­åˆ—è¡¨"""
        # æŒ‰å¥å·ã€æ„Ÿå¹å·ã€é—®å·åˆ†å‰²
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', text)
        return [s.strip() for s in sentences if s.strip()]

class SimplePoetryGenerator:
    """ç®€åŒ–çš„è¯—æ­Œç”Ÿæˆå™¨"""
    
    def __init__(self, model, ix2word: Dict[int, str], word2ix: Dict[str, int]):
        self.model = model
        self.text_processor = TextProcessor(ix2word, word2ix)
        self.sampler = TextSampler()
        self.device = next(model.parameters()).device
        self.period_id = word2ix.get('ã€‚', -1)
    
    def generate(self, prompt: str = "", max_length: int = 100,
                temperature: float = 0.8, top_k: int = 50, 
                top_p: float = 0.9, num_samples: int = 1) -> List[str]:
        """åŸºç¡€æ–‡æœ¬ç”Ÿæˆ"""
        self.model.eval()
        results = []
        
        with torch.no_grad():
            for _ in range(num_samples):
                # è½¬æ¢æç¤ºæ–‡æœ¬ä¸ºç´¢å¼•
                if prompt:
                    input_ids = self.text_processor.text_to_indices(prompt, add_special=True)
                else:
                    # ä½¿ç”¨START tokenå¼€å§‹
                    start_token = self.text_processor.word2ix.get(config.START_TOKEN, 0)
                    input_ids = [start_token]
                
                input_tensor = torch.tensor([input_ids], device=self.device)
                
                # ç”Ÿæˆ
                for _ in range(max_length):
                    # è®¡ç®—éŸµå¾‹ä½ç½®ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼Œç”¨äºç”Ÿæˆï¼‰
                    char_positions = self._compute_char_positions(input_tensor)
                    
                    outputs = self.model(input_tensor, char_positions=char_positions)
                    
                    # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                    if isinstance(outputs, dict):
                        logits = outputs['logits'][:, -1, :]
                    else:
                        logits = outputs[:, -1, :]
                    
                    # é‡‡æ ·ä¸‹ä¸€ä¸ªtoken
                    next_token = self.sampler.combined_sample(
                        logits, top_k=top_k, top_p=top_p, temperature=temperature
                    )
                    
                    # æ·»åŠ åˆ°åºåˆ—ï¼Œç¡®ä¿ç»´åº¦åŒ¹é…
                    if next_token.dim() == 0:  # æ ‡é‡
                        next_token = next_token.unsqueeze(0).unsqueeze(0)  # [1, 1]
                    elif next_token.dim() == 1:  # [batch_size]
                        next_token = next_token.unsqueeze(1)  # [batch_size, 1]
                    
                    input_tensor = torch.cat([input_tensor, next_token], dim=1)
                    
                    # æ£€æŸ¥æ˜¯å¦ç»“æŸ
                    if next_token.item() == self.text_processor.word2ix.get(config.END_TOKEN, -1):
                        break
                
                # è½¬æ¢ä¸ºæ–‡æœ¬
                generated_ids = input_tensor[0].cpu().numpy().tolist()
                text = self.text_processor.indices_to_text(generated_ids)
                text = self.text_processor.clean_text(text)
                results.append(text)
        
        return results
    
    def _compute_char_positions(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—éŸµå¾‹ä½ç½®ï¼ˆç”¨äºç”Ÿæˆï¼Œç®€åŒ–ç‰ˆæœ¬ï¼‰"""
        batch_size, seq_len = input_tensor.shape
        char_positions = torch.ones_like(input_tensor, dtype=torch.long)
        
        for i in range(batch_size):
            pos = 1
            for j in range(seq_len):
                char_positions[i, j] = pos
                if input_tensor[i, j] == self.period_id:
                    pos = 1  # é‡ç½®å¥å†…ä½ç½®
                else:
                    pos = min(pos + 1, 7)  # é™åˆ¶åœ¨1-7èŒƒå›´å†…
        
        return char_positions.to(self.device)


class ForcedLengthDecoding:
    """å¼ºåˆ¶å¥é•¿è§£ç  - æ ¸å¿ƒæœºåˆ¶2"""
    
    def __init__(self, word2ix: Dict[str, int]):
        self.word2ix = word2ix
        self.period_token = word2ix.get('ã€‚', word2ix.get('<EOP>', 0))
        
    def generate_with_constraint(self, model, start_tokens: torch.Tensor, 
                               target_length: int = 5, max_steps: int = 100,
                               temperature: float = 0.8, top_k: int = 50, top_p: float = 0.9) -> torch.Tensor:
        """
        å¸¦å¼ºåˆ¶å¥é•¿çº¦æŸçš„ç”Ÿæˆ
        
        Args:
            model: Transformeræ¨¡å‹
            start_tokens: [1, seq_len] èµ·å§‹tokens
            target_length: ç›®æ ‡å¥é•¿ï¼ˆ5=äº”è¨€ï¼Œ7=ä¸ƒè¨€ï¼‰
            max_steps: æœ€å¤§ç”Ÿæˆæ­¥æ•°
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: top-ké‡‡æ ·
            top_p: nucleusé‡‡æ ·
            
        Returns:
            generated: [1, new_seq_len] ç”Ÿæˆçš„åºåˆ—
        """
        model.eval()
        device = start_tokens.device
        generated = start_tokens.clone()
        
        char_count = 0  # å½“å‰å¥å­ä¸­çš„æ±‰å­—æ•°é‡
        
        with torch.no_grad():
            for step in range(max_steps):
                # è®¡ç®—éŸµå¾‹ä½ç½®
                char_positions = self._compute_char_positions_for_decoding(generated)
                
                # å‰å‘ä¼ æ’­
                logits = model(generated, char_positions=char_positions)
                
                # è·å–æœ€åä¸€ä¸ªä½ç½®çš„logits
                if isinstance(logits, dict):
                    next_logits = logits['logits'][0, -1, :]
                else:
                    next_logits = logits[0, -1, :]
                
                # å¼ºåˆ¶çº¦æŸé€»è¾‘
                if char_count >= target_length:
                    # å¼ºåˆ¶ä¸‹ä¸€ä¸ªtokenä¸ºå¥å·
                    next_token = self.period_token
                    char_count = 0
                    print(f"ğŸ“ å¼ºåˆ¶æ–­å¥ï¼šå·²ç”Ÿæˆ{target_length}å­—")
                else:
                    # æ­£å¸¸é‡‡æ ·
                    next_token = self._sample_token(next_logits, temperature, top_k, top_p)
                    
                    # æ£€æŸ¥æ˜¯å¦ä¸ºæ±‰å­—ï¼ˆç®€å•åˆ¤æ–­ï¼štoken idåœ¨åˆç†èŒƒå›´å†…ï¼‰
                    if self._is_chinese_char(next_token):
                        char_count += 1
                    elif next_token == self.period_token:
                        char_count = 0
                
                # æ·»åŠ åˆ°åºåˆ—
                next_token_tensor = torch.tensor([[next_token]], device=device)
                generated = torch.cat([generated, next_token_tensor], dim=1)
                
                # æ£€æŸ¥ç»“æŸæ¡ä»¶
                if next_token == self.period_token and char_count == 0:
                    # åˆšå®Œæˆä¸€å¥
                    continue
                elif generated.size(1) >= start_tokens.size(1) + max_steps:
                    break
        
        return generated
    
    def _sample_token(self, logits: torch.Tensor, temperature: float, top_k: int, top_p: float) -> int:
        """é‡‡æ ·ä¸‹ä¸€ä¸ªtoken"""
        return TextSampler.combined_sample(
            logits.unsqueeze(0), top_k=top_k, top_p=top_p, temperature=temperature
        ).item()
    
    def _is_chinese_char(self, token_id: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ±‰å­—tokenï¼ˆæ’é™¤ç‰¹æ®Šç¬¦å·å’Œæ ‡ç‚¹ï¼‰"""
        # æ’é™¤ç‰¹æ®Štokenå’Œæ ‡ç‚¹ç¬¦å·
        special_tokens = {
            self.word2ix.get('</s>', -1),
            self.word2ix.get('<START>', -1), 
            self.word2ix.get('<EOP>', -1),
            self.word2ix.get('<PAD>', -1),
            self.word2ix.get('<UNK>', -1),
            self.period_token,  # å¥å·
            self.word2ix.get('ï¼Œ', -1),  # é€—å·
            self.word2ix.get('ï¼', -1),  # æ„Ÿå¹å·
            self.word2ix.get('ï¼Ÿ', -1),  # é—®å·
            self.word2ix.get('ï¼š', -1),  # å†’å·
            self.word2ix.get('ï¼›', -1),  # åˆ†å·
            self.word2ix.get('"', -1),   # å¼•å·
            self.word2ix.get('"', -1),   # å¼•å·
            -1  # æ’é™¤-1è¿™ä¸ªæ— æ•ˆID
        }
        
        # å¦‚æœæ˜¯ç‰¹æ®Štokenï¼Œä¸æ˜¯æ±‰å­—
        if token_id in special_tokens:
            return False
            
        # ç®€å•ç­–ç•¥ï¼šå¯¹äºæµ‹è¯•ç¯å¢ƒï¼ŒID >= 4 ä¸”ä¸åœ¨ç‰¹æ®Šç¬¦å·é›†åˆä¸­çš„éƒ½è®¤ä¸ºæ˜¯æ±‰å­—
        # å¯¹äºå®é™…ç¯å¢ƒï¼Œå¯ä»¥æ ¹æ®è¯æ±‡è¡¨è§„æ¨¡è°ƒæ•´
        vocab_size = len(self.word2ix)
        if vocab_size < 100:  # æµ‹è¯•ç¯å¢ƒçš„å°è¯æ±‡è¡¨
            return token_id >= 4 and token_id not in special_tokens
        else:  # å®é™…ç¯å¢ƒçš„å¤§è¯æ±‡è¡¨
            return token_id >= 50 and token_id not in special_tokens
    
    def _compute_char_positions_for_decoding(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """ä¸ºå¼ºåˆ¶å¥é•¿è§£ç è®¡ç®—éŸµå¾‹ä½ç½®"""
        batch_size, seq_len = input_tensor.shape
        char_positions = torch.ones_like(input_tensor, dtype=torch.long)
        
        for i in range(batch_size):
            pos = 1
            for j in range(seq_len):
                char_positions[i, j] = pos
                if input_tensor[i, j] == self.period_token:
                    pos = 1  # é‡ç½®å¥å†…ä½ç½®
                else:
                    pos = min(pos + 1, 7)  # é™åˆ¶åœ¨1-7èŒƒå›´å†…
        
        return char_positions.to(input_tensor.device)


class QualityAssessment:
    """ç”Ÿæˆè´¨é‡è¯„ä¼°å·¥å…·"""
    
    def __init__(self, ix2word: Dict[int, str], word2ix: Dict[str, int]):
        self.ix2word = ix2word
        self.word2ix = word2ix
        self.processor = TextProcessor(ix2word, word2ix)
    
    def assess_poem_quality(self, poem_text: str, poem_type: str = "ç»­å†™") -> Dict[str, float]:
        """è¯„ä¼°è¯—æ­Œè´¨é‡"""
        sentences = self.processor.extract_sentences(poem_text)
        
        metrics = {
            'length_score': self._assess_length(sentences),
            'structure_score': self._assess_structure(sentences),
            'repetition_score': self._assess_repetition(poem_text),
            'coherence_score': self._assess_coherence(sentences),
            'overall_score': 0.0
        }
        
        # è®¡ç®—æ€»åˆ†
        weights = [0.2, 0.3, 0.3, 0.2]
        score_values = [metrics['length_score'], metrics['structure_score'], 
                       metrics['repetition_score'], metrics['coherence_score']]
        metrics['overall_score'] = sum(w * s for w, s in zip(weights, score_values))
        
        return metrics
    
    def _assess_length(self, sentences: List[str]) -> float:
        """è¯„ä¼°å¥å­é•¿åº¦åˆç†æ€§"""
        if not sentences:
            return 0.0
        
        lengths = [len(s) for s in sentences]
        
        # ç†æƒ³é•¿åº¦èŒƒå›´ï¼š5-7å­—
        ideal_range = (5, 7)
        score = 0.0
        
        for length in lengths:
            if ideal_range[0] <= length <= ideal_range[1]:
                score += 1.0
            elif abs(length - ideal_range[0]) <= 2 or abs(length - ideal_range[1]) <= 2:
                score += 0.5
        
        return score / len(sentences) if sentences else 0.0
    
    def _assess_structure(self, sentences: List[str]) -> float:
        """è¯„ä¼°è¯—æ­Œç»“æ„"""
        if len(sentences) < 2:
            return 0.3
        elif 2 <= len(sentences) <= 4:
            return 1.0
        elif 5 <= len(sentences) <= 8:
            return 0.8
        else:
            return 0.5
    
    def _assess_repetition(self, text: str) -> float:
        """è¯„ä¼°é‡å¤åº¦ï¼ˆè¶Šä½è¶Šå¥½ï¼‰"""
        if not text:
            return 0.0
        
        char_counts = Counter(text)
        text_length = len(text)
        
        # è®¡ç®—å­—ç¬¦é‡å¤ç‡
        repetition_rate = sum(count - 1 for count in char_counts.values() if count > 1) / text_length
        
        # è½¬æ¢ä¸ºåˆ†æ•°ï¼ˆé‡å¤ç‡è¶Šä½åˆ†æ•°è¶Šé«˜ï¼‰
        return max(0.0, 1.0 - repetition_rate * 2)
    
    def _assess_coherence(self, sentences: List[str]) -> float:
        """è¯„ä¼°è¯­ä¹‰è¿è´¯æ€§ï¼ˆç®€å•å¯å‘å¼ï¼‰"""
        if len(sentences) < 2:
            return 0.7
        
        # æ£€æŸ¥å¥å­é•¿åº¦çš„ä¸€è‡´æ€§
        lengths = [len(s) for s in sentences]
        length_variance = np.var(lengths) if len(lengths) > 1 else 0
        
        # é•¿åº¦ä¸€è‡´æ€§åˆ†æ•°
        length_score = max(0.0, 1.0 - length_variance / 10)
        
        return length_score

def create_quality_assessor(ix2word: Dict[int, str], word2ix: Dict[str, int]) -> QualityAssessment:
    """åˆ›å»ºè´¨é‡è¯„ä¼°å™¨å®ä¾‹"""
    return QualityAssessment(ix2word, word2ix)

if __name__ == "__main__":
    # æµ‹è¯•ç”Ÿæˆå·¥å…·
    print("Testing generation utilities...")
    
    # æµ‹è¯•é‡‡æ ·ç­–ç•¥
    vocab_size = 1000
    logits = torch.randn(1, vocab_size)
    
    print("Testing sampling strategies:")
    
    # è´ªå©ªé‡‡æ ·
    greedy_result = TextSampler.greedy_sample(logits)
    print(f"Greedy sample: {greedy_result.item()}")
    
    # Top-Ké‡‡æ ·
    topk_result = TextSampler.top_k_sample(logits, k=50)
    print(f"Top-K sample: {topk_result.item()}")
    
    # Nucleusé‡‡æ ·
    nucleus_result = TextSampler.nucleus_sample(logits, p=0.9)
    print(f"Nucleus sample: {nucleus_result.item()}")
    
    # ç»„åˆé‡‡æ ·
    combined_result = TextSampler.combined_sample(logits)
    print(f"Combined sample: {combined_result.item()}")
    
    # æµ‹è¯•æ–‡æœ¬å¤„ç†
    print("\nTesting text processing:")
    
    # åˆ›å»ºç¤ºä¾‹è¯æ±‡è¡¨
    test_ix2word = {0: '</s>', 1: '<START>', 2: '<EOP>', 3: 'æ˜¥', 4: 'èŠ±', 5: 'ç§‹', 6: 'æœˆ', 7: 'ã€‚'}
    test_word2ix = {v: k for k, v in test_ix2word.items()}
    
    processor = TextProcessor(test_ix2word, test_word2ix)
    
    # æµ‹è¯•ç´¢å¼•åˆ°æ–‡æœ¬
    test_indices = [1, 3, 4, 7, 5, 6, 7, 2]
    text = processor.indices_to_text(test_indices)
    print(f"Indices to text: {text}")
    
    # æµ‹è¯•æ–‡æœ¬åˆ°ç´¢å¼•
    test_text = "æ˜¥èŠ±ã€‚ç§‹æœˆã€‚"
    indices = processor.text_to_indices(test_text)
    print(f"Text to indices: {indices}")
    
    # æµ‹è¯•æ ¼å¼åŒ–
    formatted = processor.format_poem(text, "æµ‹è¯•")
    print(f"Formatted poem:\n{formatted}")
    
    # æµ‹è¯•è´¨é‡è¯„ä¼°
    print("\nTesting quality assessment:")
    assessor = QualityAssessment(test_ix2word, test_word2ix)
    quality = assessor.assess_poem_quality("æ˜¥èŠ±ç§‹æœˆå¤œã€‚æ˜æœˆç…§äººå½’ã€‚")
    print(f"Quality metrics: {quality}")
    
    print("\nGeneration utilities testing completed!")

class ConstrainedPoetryGenerator:
    """çº¦æŸè§£ç è¯—æ­Œç”Ÿæˆå™¨ - ä¸¥æ ¼éµå¾ªå”è¯—æ ¼å¾‹"""
    
    def __init__(self, model, ix2word: Dict[int, str], word2ix: Dict[str, int]):
        self.model = model
        self.text_processor = TextProcessor(ix2word, word2ix)
        self.sampler = TextSampler()
        self.device = next(model.parameters()).device
        self.word2ix = word2ix
        self.ix2word = ix2word
        
        # é‡è¦çš„token ID
        self.period_id = word2ix.get('ã€‚', -1)
        self.comma_id = word2ix.get('ï¼Œ', -1)
        self.start_id = word2ix.get(config.START_TOKEN, 0)
        self.end_id = word2ix.get(config.END_TOKEN, -1)
        
        # æ ‡ç‚¹ç¬¦å·é›†åˆ
        self.punctuation_ids = {
            self.period_id, self.comma_id,
            word2ix.get('ï¼', -1), word2ix.get('ï¼Ÿ', -1),
            word2ix.get('ï¼š', -1), word2ix.get('ï¼›', -1),
            word2ix.get('"', -1), word2ix.get('"', -1),
            word2ix.get('</s>', -1), word2ix.get('<START>', -1),
            word2ix.get('<EOP>', -1), -1
        }
    
    def generate_constrained_poem(self, prompt: str = "", poem_type: str = "ä¸ƒè¨€ç»å¥",
                                temperature: float = 0.8, top_k: int = 50, 
                                top_p: float = 0.9) -> str:
        """
        ç”Ÿæˆä¸¥æ ¼ç¬¦åˆæ ¼å¾‹çš„è¯—æ­Œ
        
        Args:
            prompt: æç¤ºè¯
            poem_type: è¯—æ­Œç±»å‹ ("äº”è¨€ç»å¥", "ä¸ƒè¨€ç»å¥", "äº”è¨€å¾‹è¯—", "ä¸ƒè¨€å¾‹è¯—")
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: top-ké‡‡æ ·
            top_p: nucleusé‡‡æ ·
            
        Returns:
            ç¬¦åˆæ ¼å¾‹çš„è¯—æ­Œæ–‡æœ¬
        """
        # è§£æè¯—æ­Œç±»å‹
        char_per_line, total_lines = self._parse_poem_type(poem_type)
        
        self.model.eval()
        
        with torch.no_grad():
            # åˆå§‹åŒ–è¾“å…¥
            if prompt:
                input_ids = self.text_processor.text_to_indices(prompt, add_special=True)
            else:
                input_ids = [self.start_id]
            
            input_tensor = torch.tensor([input_ids], device=self.device)
            
            # ç”Ÿæˆè®¡æ•°å™¨
            current_line_chars = 0  # å½“å‰å¥ä¸­çš„æ±‰å­—æ•°
            completed_lines = 0     # å·²å®Œæˆçš„å¥æ•°
            
            # å¦‚æœæœ‰æç¤ºè¯ï¼Œè®¡ç®—å…¶å­—ç¬¦æ•°
            if prompt:
                current_line_chars = self._count_chinese_chars(prompt)
            
            # ä¸»ç”Ÿæˆå¾ªç¯
            while completed_lines < total_lines:
                # è®¡ç®—éŸµå¾‹ä½ç½®
                char_positions = self._compute_char_positions(input_tensor)
                
                # å‰å‘ä¼ æ’­
                outputs = self.model(input_tensor, char_positions=char_positions)
                
                if isinstance(outputs, dict):
                    logits = outputs['logits'][:, -1, :]
                else:
                    logits = outputs[:, -1, :]
                
                # çº¦æŸè§£ç é€»è¾‘
                next_token = self._constrained_sampling(
                    logits, current_line_chars, char_per_line, 
                    completed_lines, total_lines, temperature, top_k, top_p
                )
                
                # æ›´æ–°è®¡æ•°å™¨
                if self._is_chinese_char(next_token):
                    current_line_chars += 1
                elif next_token in [self.period_id, self.comma_id]:
                    completed_lines += 1
                    current_line_chars = 0
                
                # æ·»åŠ tokenåˆ°åºåˆ—
                next_token_tensor = torch.tensor([[next_token]], device=self.device)
                input_tensor = torch.cat([input_tensor, next_token_tensor], dim=1)
                
                # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ— é™å¾ªç¯
                if input_tensor.size(1) > 200:
                    break
            
            # è½¬æ¢ä¸ºæ–‡æœ¬
            generated_ids = input_tensor[0].cpu().numpy().tolist()
            text = self.text_processor.indices_to_text(generated_ids)
            return self._format_poem(text, char_per_line)
    
    def _parse_poem_type(self, poem_type: str) -> tuple:
        """è§£æè¯—æ­Œç±»å‹ï¼Œè¿”å›(æ¯å¥å­—æ•°, æ€»å¥æ•°)"""
        type_map = {
            "äº”è¨€ç»å¥": (5, 4),
            "ä¸ƒè¨€ç»å¥": (7, 4), 
            "äº”è¨€å¾‹è¯—": (5, 8),
            "ä¸ƒè¨€å¾‹è¯—": (7, 8)
        }
        return type_map.get(poem_type, (7, 4))  # é»˜è®¤ä¸ƒè¨€ç»å¥
    
    def _constrained_sampling(self, logits: torch.Tensor, current_chars: int, 
                            target_chars: int, completed_lines: int, total_lines: int,
                            temperature: float, top_k: int, top_p: float) -> int:
        """çº¦æŸé‡‡æ ·ï¼šæ ¹æ®å½“å‰çŠ¶æ€å†³å®šä¸‹ä¸€ä¸ªtoken"""
        
        # è§„åˆ™1ï¼šå¦‚æœå½“å‰å¥å·²è¾¾åˆ°ç›®æ ‡å­—æ•°ï¼Œå¼ºåˆ¶ç”Ÿæˆæ­£ç¡®çš„æ ‡ç‚¹ç¬¦å·
        if current_chars >= target_chars:
            # æ ¹æ®å”è¯—æ ¼å¾‹ç¡®å®šæ ‡ç‚¹ç¬¦å·
            return self._get_correct_punctuation(completed_lines, total_lines)
        
        # è§„åˆ™2ï¼šå¦‚æœå·²å®Œæˆæ‰€æœ‰å¥å­ï¼Œå¼ºåˆ¶ç»“æŸ
        if completed_lines >= total_lines:
            return self.end_id
        
        # è§„åˆ™3ï¼šæ­£å¸¸é‡‡æ ·ï¼Œä½†æ’é™¤å¥å·ï¼ˆé™¤éæ»¡è¶³æ¡ä»¶ï¼‰
        # åˆ›å»ºæ©ç ï¼Œç¦æ­¢åœ¨ä¸åˆé€‚çš„ä½ç½®ç”Ÿæˆå¥å·
        masked_logits = logits.clone()
        
        # åœ¨å­—æ•°ä¸è¶³æ—¶ï¼Œå¤§å¹…é™ä½æ‰€æœ‰æ ‡ç‚¹ç¬¦å·çš„æ¦‚ç‡
        if current_chars < target_chars:
            masked_logits[0, self.period_id] = float('-inf')
            masked_logits[0, self.comma_id] = float('-inf')
        
        # åœ¨æœ€åä¸€å¥å®Œæˆåï¼Œç¦æ­¢ç»§ç»­ç”Ÿæˆæ±‰å­—
        if completed_lines >= total_lines - 1 and current_chars >= target_chars:
            # åªå…è®¸å¥å·
            for token_id in range(len(self.word2ix)):
                if token_id != self.period_id:
                    masked_logits[0, token_id] = float('-inf')
        
        # ä½¿ç”¨çº¦æŸåçš„logitsè¿›è¡Œé‡‡æ ·
        next_token = self.sampler.combined_sample(
            masked_logits, top_k=top_k, top_p=top_p, temperature=temperature
        )
        
        return next_token.item()
    
    def _get_correct_punctuation(self, completed_lines: int, total_lines: int) -> int:
        """æ ¹æ®å”è¯—æ ¼å¾‹è·å–æ­£ç¡®çš„æ ‡ç‚¹ç¬¦å·"""
        # completed_lines æ˜¯å½“å‰æ­£åœ¨å®Œæˆçš„å¥å­ç´¢å¼•ï¼ˆ0-basedï¼‰
        
        if total_lines == 4:  # ç»å¥ï¼š1,3å¥é€—å·ï¼Œ2,4å¥å¥å·
            if completed_lines in [0, 2]:  # ç¬¬1,3å¥ï¼ˆ0,2ç´¢å¼•ï¼‰
                return self.comma_id
            else:  # ç¬¬2,4å¥ï¼ˆ1,3ç´¢å¼•ï¼‰
                return self.period_id
        
        elif total_lines == 8:  # å¾‹è¯—ï¼šå‰6å¥é€—å·ï¼Œç¬¬7å¥é€—å·ï¼Œç¬¬8å¥å¥å·
            if completed_lines < 7:  # å‰7å¥ï¼ˆç´¢å¼•0-6ï¼‰
                return self.comma_id
            else:  # ç¬¬8å¥ï¼ˆç´¢å¼•7ï¼‰
                return self.period_id
        
        else:  # å…¶ä»–æƒ…å†µï¼Œé»˜è®¤æœ€åä¸€å¥å¥å·ï¼Œå…¶ä½™é€—å·
            if completed_lines == total_lines - 1:
                return self.period_id
            else:
                return self.comma_id
    
    def _is_chinese_char(self, token_id: int) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ±‰å­—token"""
        # æ’é™¤æ ‡ç‚¹ç¬¦å·å’Œç‰¹æ®Štoken
        if token_id in self.punctuation_ids:
            return False
        
        # ç®€å•ç­–ç•¥ï¼šè®¤ä¸ºå¤§éƒ¨åˆ†tokenéƒ½æ˜¯æ±‰å­—
        vocab_size = len(self.word2ix)
        if vocab_size > 1000:  # å¤§è¯æ±‡è¡¨
            return token_id >= 100 and token_id not in self.punctuation_ids
        else:  # å°è¯æ±‡è¡¨
            return token_id >= 10 and token_id not in self.punctuation_ids
    
    def _count_chinese_chars(self, text: str) -> int:
        """ç»Ÿè®¡æ–‡æœ¬ä¸­çš„æ±‰å­—æ•°é‡"""
        count = 0
        for char in text:
            if '\u4e00' <= char <= '\u9fff':  # Unicodeæ±‰å­—èŒƒå›´
                count += 1
        return count
    
    def _compute_char_positions(self, input_tensor: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—éŸµå¾‹ä½ç½®ï¼ˆç”¨äºçº¦æŸç”Ÿæˆï¼‰"""
        batch_size, seq_len = input_tensor.shape
        char_positions = torch.ones_like(input_tensor, dtype=torch.long)
        
        for i in range(batch_size):
            pos = 1
            for j in range(seq_len):
                char_positions[i, j] = pos
                if input_tensor[i, j] == self.period_id:
                    pos = 1  # é‡ç½®å¥å†…ä½ç½®
                else:
                    pos = min(pos + 1, 7)  # é™åˆ¶åœ¨1-7èŒƒå›´å†…
        
        return char_positions.to(self.device)
    
    def _format_poem(self, text: str, chars_per_line: int) -> str:
        """æ ¼å¼åŒ–è¯—æ­Œè¾“å‡º"""
        # ç§»é™¤ç‰¹æ®Šæ ‡è®°
        text = text.replace('<START>', '').replace('<EOP>', '').replace('</s>', '')
        
        # æŒ‰å¥å·å’Œé€—å·åˆ†å‰²ï¼Œä¿æŒåŸæœ‰æ ‡ç‚¹
        import re
        lines = re.split(r'[ã€‚ï¼Œ]', text)
        formatted_lines = []
        
        # æå–åŸå§‹æ ‡ç‚¹ç¬¦å·
        punctuation_marks = re.findall(r'[ã€‚ï¼Œ]', text)
        
        for i, line in enumerate(lines):
            line = line.strip()
            if line:  # éç©ºè¡Œ
                # ç¡®ä¿æ¯è¡Œå­—æ•°æ­£ç¡®ï¼ˆåå¤„ç†ä¿æŠ¤ï¼‰
                chinese_chars = [c for c in line if '\u4e00' <= c <= '\u9fff']
                if len(chinese_chars) > chars_per_line:
                    # æˆªå–åˆ°ç›®æ ‡å­—æ•°
                    line = ''.join(chinese_chars[:chars_per_line])
                elif len(chinese_chars) < chars_per_line:
                    # å­—æ•°ä¸è¶³çš„æƒ…å†µï¼Œä¿æŒåŸæ ·ï¼ˆåœ¨ç”Ÿæˆæ—¶åº”è¯¥å·²ç»é¿å…ï¼‰
                    line = ''.join(chinese_chars)
                else:
                    line = ''.join(chinese_chars)
                
                if line:
                    # æ·»åŠ å¯¹åº”çš„æ ‡ç‚¹ç¬¦å·
                    if i < len(punctuation_marks):
                        formatted_lines.append(line + punctuation_marks[i])
                    else:
                        formatted_lines.append(line + 'ã€‚')  # é»˜è®¤å¥å·
        
        return '\n'.join(formatted_lines)
    
    def generate_acrostic_poem(self, acrostic_chars: str, poem_type: str = "äº”è¨€ç»å¥",
                             temperature: float = 0.8, top_k: int = 50, 
                             top_p: float = 0.9) -> str:
        """
        ç”Ÿæˆè—å¤´è¯—
        
        Args:
            acrostic_chars: è—å¤´å­—ç¬¦ä¸²ï¼Œæ¯ä¸ªå­—ç¬¦å¯¹åº”ä¸€å¥çš„é¦–å­—
            poem_type: è¯—æ­Œç±»å‹ ("äº”è¨€ç»å¥", "ä¸ƒè¨€ç»å¥", "äº”è¨€å¾‹è¯—", "ä¸ƒè¨€å¾‹è¯—")
            temperature: é‡‡æ ·æ¸©åº¦
            top_k: top-ké‡‡æ ·
            top_p: nucleusé‡‡æ ·
            
        Returns:
            è—å¤´è¯—æ–‡æœ¬
        """
        # è§£æè¯—æ­Œç±»å‹
        char_per_line, total_lines = self._parse_poem_type(poem_type)
        
        # æ£€æŸ¥è—å¤´å­—ç¬¦æ•°é‡ä¸è¯—ä½“æ˜¯å¦åŒ¹é…
        if len(acrostic_chars) != total_lines:
            # å¦‚æœå­—ç¬¦æ•°ä¸åŒ¹é…ï¼Œè°ƒæ•´å­—ç¬¦ä¸²
            if len(acrostic_chars) < total_lines:
                # å­—ç¬¦æ•°ä¸è¶³ï¼Œé‡å¤æœ€åä¸€ä¸ªå­—ç¬¦
                acrostic_chars = acrostic_chars + acrostic_chars[-1] * (total_lines - len(acrostic_chars))
            else:
                # å­—ç¬¦æ•°è¿‡å¤šï¼Œæˆªå–å‰é¢çš„å­—ç¬¦
                acrostic_chars = acrostic_chars[:total_lines]
        
        self.model.eval()
        
        all_lines = []
        
        with torch.no_grad():
            # é€å¥ç”Ÿæˆè—å¤´è¯—
            for line_idx, first_char in enumerate(acrostic_chars):
                # æ¯å¥éƒ½ä»¥è—å¤´å­—ç¬¦å¼€å§‹
                first_char_id = self.word2ix.get(first_char)
                if first_char_id is None:
                    # å¦‚æœè—å¤´å­—ç¬¦ä¸åœ¨è¯æ±‡è¡¨ä¸­ï¼Œè·³è¿‡è¿™å¥
                    continue
                
                # åˆå§‹åŒ–è¾“å…¥ï¼šSTART + è—å¤´å­—ç¬¦
                input_ids = [self.start_id, first_char_id]
                input_tensor = torch.tensor([input_ids], device=self.device)
                
                # ç”Ÿæˆè¿™ä¸€å¥çš„å‰©ä½™éƒ¨åˆ†ï¼ˆéœ€è¦ char_per_line - 1 ä¸ªæ±‰å­— + æ ‡ç‚¹ï¼‰
                current_chars = 1  # å·²æœ‰è—å¤´å­—ç¬¦
                
                while current_chars < char_per_line:
                    # è®¡ç®—éŸµå¾‹ä½ç½®
                    char_positions = self._compute_char_positions(input_tensor)
                    
                    # å‰å‘ä¼ æ’­
                    outputs = self.model(input_tensor, char_positions=char_positions)
                    
                    if isinstance(outputs, dict):
                        logits = outputs['logits'][:, -1, :]
                    else:
                        logits = outputs[:, -1, :]
                    
                    # é‡‡æ ·ä¸‹ä¸€ä¸ªtokenï¼ˆé¿å…æ ‡ç‚¹ç¬¦å·ï¼‰
                    masked_logits = logits.clone()
                    
                    # åœ¨å¥å­æœªå®Œæˆæ—¶ï¼Œç¦æ­¢ç”Ÿæˆæ ‡ç‚¹ç¬¦å·
                    if current_chars < char_per_line:
                        masked_logits[0, self.period_id] = float('-inf')
                        masked_logits[0, self.comma_id] = float('-inf')
                    
                    next_token = self.sampler.combined_sample(
                        masked_logits, top_k=top_k, top_p=top_p, temperature=temperature
                    )
                    
                    next_token_id = next_token.item()
                    
                    # åªç»Ÿè®¡æ±‰å­—
                    if self._is_chinese_char(next_token_id):
                        current_chars += 1
                    
                    # æ·»åŠ tokenåˆ°åºåˆ—
                    next_token_tensor = torch.tensor([[next_token_id]], device=self.device)
                    input_tensor = torch.cat([input_tensor, next_token_tensor], dim=1)
                    
                    # å®‰å…¨æ£€æŸ¥ï¼šé˜²æ­¢æ— é™å¾ªç¯
                    if input_tensor.size(1) > 50:
                        break
                
                # æ·»åŠ æ­£ç¡®çš„æ ‡ç‚¹ç¬¦å·
                correct_punctuation = self._get_correct_punctuation(line_idx, total_lines)
                punctuation_tensor = torch.tensor([[correct_punctuation]], device=self.device)
                input_tensor = torch.cat([input_tensor, punctuation_tensor], dim=1)
                
                # è½¬æ¢è¿™ä¸€å¥ä¸ºæ–‡æœ¬
                line_ids = input_tensor[0].cpu().numpy().tolist()
                line_text = self.text_processor.indices_to_text(line_ids)
                
                # æ¸…ç†å¹¶æ ¼å¼åŒ–è¿™ä¸€å¥
                line_text = line_text.replace('<START>', '').replace('<EOP>', '').replace('</s>', '')
                all_lines.append(line_text.strip())
        
        # ç»„åˆæ‰€æœ‰å¥å­
        result = '\n'.join(all_lines)
        return result