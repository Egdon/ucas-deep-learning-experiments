#!/usr/bin/env python3
"""
æ¨¡å‹ç¿»è¯‘æ•ˆæœæµ‹è¯•è„šæœ¬
ç”¨äºå¿«é€Ÿæ£€éªŒè‹±è¯‘ä¸­æ¨¡å‹çš„ç¿»è¯‘è´¨é‡
"""

import torch
import numpy as np
import json
import random
import argparse
import os
from datetime import datetime

import config
from model import make_model, batch_greedy_decode
from utils import english_tokenizer_load, chinese_tokenizer_load
from data_loader import MTDataset


class TranslationTester:
    """ç¿»è¯‘æµ‹è¯•å™¨"""
    
    def __init__(self, model_path=None):
        self.model_path = model_path or config.model_path
        self.device = config.device
        
        # æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦å­˜åœ¨
        if not os.path.exists(self.model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")
        
        print(f"ğŸ”„ æ­£åœ¨åŠ è½½æ¨¡å‹: {self.model_path}")
        
        # åŠ è½½åˆ†è¯å™¨
        self.en_tokenizer = english_tokenizer_load()
        self.cn_tokenizer = chinese_tokenizer_load()
        
        # ç‰¹æ®Štoken
        self.BOS = self.en_tokenizer.bos_id()  # 2
        self.EOS = self.en_tokenizer.eos_id()  # 3
        self.PAD = self.en_tokenizer.pad_id()  # 0
        
        # åˆ›å»ºå¹¶åŠ è½½æ¨¡å‹
        self.model = make_model(
            config.src_vocab_size, config.tgt_vocab_size,
            config.n_layers, config.d_model, config.d_ff,
            config.n_heads, config.dropout
        )
        
        # åŠ è½½è®­ç»ƒå¥½çš„å‚æ•°
        self.model.load_state_dict(torch.load(self.model_path, map_location=self.device))
        self.model.eval()
        
        print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ! å‚æ•°é‡: {sum(p.numel() for p in self.model.parameters()) / 1000000.0:.1f}M")
    
    def translate_sentence(self, english_text, max_len=60):
        """ç¿»è¯‘å•ä¸ªè‹±æ–‡å¥å­"""
        with torch.no_grad():
            # åˆ†è¯å¹¶æ·»åŠ ç‰¹æ®Štoken
            src_tokens = [self.BOS] + self.en_tokenizer.EncodeAsIds(english_text) + [self.EOS]
            src_tensor = torch.LongTensor([src_tokens]).to(self.device)
            src_mask = (src_tensor != self.PAD).unsqueeze(-2)
            
            # è´ªå¿ƒè§£ç 
            result = batch_greedy_decode(self.model, src_tensor, src_mask, max_len=max_len)
            
            # è§£ç ä¸ºä¸­æ–‡æ–‡æœ¬
            if result and len(result) > 0:
                chinese_text = self.cn_tokenizer.decode_ids(result[0])
                return chinese_text
            else:
                return "ç¿»è¯‘å¤±è´¥"
    
    def translate_batch(self, english_texts, max_len=60):
        """æ‰¹é‡ç¿»è¯‘è‹±æ–‡å¥å­"""
        results = []
        
        with torch.no_grad():
            # å‡†å¤‡æ‰¹é‡æ•°æ®
            batch_tokens = []
            for text in english_texts:
                src_tokens = [self.BOS] + self.en_tokenizer.EncodeAsIds(text) + [self.EOS]
                batch_tokens.append(src_tokens)
            
            # å¡«å……åˆ°ç›¸åŒé•¿åº¦
            max_src_len = max(len(tokens) for tokens in batch_tokens)
            padded_batch = []
            for tokens in batch_tokens:
                padded_tokens = tokens + [self.PAD] * (max_src_len - len(tokens))
                padded_batch.append(padded_tokens)
            
            # è½¬æ¢ä¸ºtensor
            src_tensor = torch.LongTensor(padded_batch).to(self.device)
            src_mask = (src_tensor != self.PAD).unsqueeze(-2)
            
            # æ‰¹é‡è§£ç 
            decode_results = batch_greedy_decode(self.model, src_tensor, src_mask, max_len=max_len)
            
            # è½¬æ¢ä¸ºä¸­æ–‡æ–‡æœ¬
            for result in decode_results:
                if result:
                    chinese_text = self.cn_tokenizer.decode_ids(result)
                    results.append(chinese_text)
                else:
                    results.append("ç¿»è¯‘å¤±è´¥")
        
        return results
    
    def test_sample_sentences(self):
        """æµ‹è¯•ä¸€äº›æ ·ä¾‹å¥å­"""
        test_samples = [
            {
                "english": "Hello, how are you?",
                "reference": "ä½ å¥½ï¼Œä½ å¥½å—ï¼Ÿ"
            },
            {
                "english": "I love artificial intelligence and machine learning.",
                "reference": "æˆ‘å–œæ¬¢äººå·¥æ™ºèƒ½å’Œæœºå™¨å­¦ä¹ ã€‚"
            },
            {
                "english": "The weather is very nice today.",
                "reference": "ä»Šå¤©å¤©æ°”å¾ˆå¥½ã€‚"
            },
            {
                "english": "Can you help me with this problem?",
                "reference": "ä½ èƒ½å¸®æˆ‘è§£å†³è¿™ä¸ªé—®é¢˜å—ï¼Ÿ"
            },
            {
                "english": "Technology is changing our world rapidly.",
                "reference": "æŠ€æœ¯æ­£åœ¨è¿…é€Ÿæ”¹å˜æˆ‘ä»¬çš„ä¸–ç•Œã€‚"
            },
            {
                "english": "The near-term policy remedies are clear: raise the minimum wage to a level that will keep a fully employed worker and his or her family out of poverty.",
                "reference": "è¿‘æœŸçš„æ”¿ç­–å¯¹ç­–å¾ˆæ˜ç¡®ï¼šæŠŠæœ€ä½å·¥èµ„æå‡åˆ°è¶³ä»¥ä¸€ä¸ªå…¨èŒå·¥äººåŠå…¶å®¶åº­å…äºè´«å›°çš„æ°´å¹³ã€‚"
            }
        ]
        
        print("=" * 80)
        print("ğŸ” æ ·ä¾‹å¥å­ç¿»è¯‘æµ‹è¯•")
        print("=" * 80)
        
        for i, sample in enumerate(test_samples, 1):
            english = sample["english"]
            reference = sample["reference"]
            
            print(f"\nã€æ ·ä¾‹ {i}ã€‘")
            print(f"åŸæ–‡: {english}")
            print(f"å‚è€ƒ: {reference}")
            
            # ç¿»è¯‘
            translation = self.translate_sentence(english)
            print(f"è¯‘æ–‡: {translation}")
            
            # ç®€å•è´¨é‡è¯„ä¼°
            if translation and translation != "ç¿»è¯‘å¤±è´¥":
                print("âœ… ç¿»è¯‘æˆåŠŸ")
            else:
                print("âŒ ç¿»è¯‘å¤±è´¥")
            
            print("-" * 60)
    
    def test_dataset_samples(self, num_samples=10):
        """ä»æµ‹è¯•æ•°æ®é›†ä¸­éšæœºé€‰æ‹©æ ·æœ¬è¿›è¡Œæµ‹è¯•"""
        print("=" * 80)
        print(f"ğŸ“Š æ•°æ®é›†éšæœºæ ·æœ¬æµ‹è¯• (å…±{num_samples}ä¸ªæ ·æœ¬)")
        print("=" * 80)
        
        try:
            # åŠ è½½æµ‹è¯•æ•°æ®é›†
            test_dataset = MTDataset(config.test_data_path)
            
            # éšæœºé€‰æ‹©æ ·æœ¬
            indices = random.sample(range(len(test_dataset)), min(num_samples, len(test_dataset)))
            
            success_count = 0
            
            for i, idx in enumerate(indices, 1):
                sample = test_dataset[idx]
                english = sample[0]
                reference = sample[1]
                
                print(f"\nã€æµ‹è¯•æ ·æœ¬ {i}ã€‘(æ•°æ®é›†ç´¢å¼•: {idx})")
                print(f"åŸæ–‡: {english}")
                print(f"å‚è€ƒ: {reference}")
                
                # ç¿»è¯‘
                translation = self.translate_sentence(english)
                print(f"è¯‘æ–‡: {translation}")
                
                if translation and translation != "ç¿»è¯‘å¤±è´¥":
                    success_count += 1
                    print("âœ… ç¿»è¯‘æˆåŠŸ")
                else:
                    print("âŒ ç¿»è¯‘å¤±è´¥")
                
                print("-" * 60)
            
            print(f"\nğŸ“ˆ æˆåŠŸç‡: {success_count}/{num_samples} ({success_count/num_samples*100:.1f}%)")
            
        except Exception as e:
            print(f"âŒ æµ‹è¯•æ•°æ®é›†æ ·æœ¬æ—¶å‡ºé”™: {e}")
    
    def interactive_translation(self):
        """äº¤äº’å¼ç¿»è¯‘"""
        print("=" * 80)
        print("ğŸ’¬ äº¤äº’å¼ç¿»è¯‘æ¨¡å¼")
        print("è¾“å…¥è‹±æ–‡å¥å­è¿›è¡Œç¿»è¯‘ï¼Œè¾“å…¥ 'quit' æˆ– 'exit' é€€å‡º")
        print("=" * 80)
        
        while True:
            try:
                english_text = input("\nè¯·è¾“å…¥è‹±æ–‡å¥å­: ").strip()
                
                if english_text.lower() in ['quit', 'exit', 'q']:
                    print("ğŸ‘‹ é€€å‡ºäº¤äº’å¼ç¿»è¯‘æ¨¡å¼")
                    break
                
                if not english_text:
                    continue
                
                # ç¿»è¯‘
                translation = self.translate_sentence(english_text)
                print(f"ä¸­æ–‡è¯‘æ–‡: {translation}")
                
            except KeyboardInterrupt:
                print("\nğŸ‘‹ é€€å‡ºäº¤äº’å¼ç¿»è¯‘æ¨¡å¼")
                break
            except Exception as e:
                print(f"âŒ ç¿»è¯‘å‡ºé”™: {e}")
    
    def save_test_results(self, results, filename=None):
        """ä¿å­˜æµ‹è¯•ç»“æœåˆ°æ–‡ä»¶"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"./experiment/translation_test_{timestamp}.json"
        
        os.makedirs(os.path.dirname(filename), exist_ok=True)
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ“ æµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {filename}")


def main():
    parser = argparse.ArgumentParser(description='æ¨¡å‹ç¿»è¯‘æ•ˆæœæµ‹è¯•')
    parser.add_argument('--model', default=None, help='æ¨¡å‹æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--samples', action='store_true', help='æµ‹è¯•é¢„å®šä¹‰æ ·ä¾‹')
    parser.add_argument('--dataset', type=int, default=0, help='æµ‹è¯•æ•°æ®é›†éšæœºæ ·æœ¬æ•°é‡')
    parser.add_argument('--interactive', action='store_true', help='äº¤äº’å¼ç¿»è¯‘æ¨¡å¼')
    parser.add_argument('--sentence', type=str, help='ç¿»è¯‘æŒ‡å®šå¥å­')
    parser.add_argument('--all', action='store_true', help='è¿è¡Œæ‰€æœ‰æµ‹è¯•')
    
    args = parser.parse_args()
    
    try:
        # åˆ›å»ºç¿»è¯‘æµ‹è¯•å™¨
        tester = TranslationTester(args.model)
        
        if args.sentence:
            # ç¿»è¯‘æŒ‡å®šå¥å­
            print(f"åŸæ–‡: {args.sentence}")
            translation = tester.translate_sentence(args.sentence)
            print(f"è¯‘æ–‡: {translation}")
        
        elif args.samples or args.all:
            # æµ‹è¯•æ ·ä¾‹å¥å­
            tester.test_sample_sentences()
        
        if args.dataset > 0 or args.all:
            # æµ‹è¯•æ•°æ®é›†æ ·æœ¬
            num_samples = args.dataset if args.dataset > 0 else 10
            tester.test_dataset_samples(num_samples)
        
        if args.interactive:
            # äº¤äº’å¼ç¿»è¯‘
            tester.interactive_translation()
        
        if not any([args.samples, args.dataset, args.interactive, args.sentence, args.all]):
            # é»˜è®¤è¿è¡Œæ ·ä¾‹æµ‹è¯•
            tester.test_sample_sentences()
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 