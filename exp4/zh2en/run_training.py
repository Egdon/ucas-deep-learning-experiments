#!/usr/bin/env python3
"""
ä¸­è¯‘è‹±æ¨¡å‹è®­ç»ƒè¿è¡Œè„šæœ¬
åŒ…å«ç¯å¢ƒæ£€æŸ¥ã€GPUä¿¡æ¯æ˜¾ç¤ºå’Œæ¨¡å‹è®­ç»ƒå¯åŠ¨
"""

import torch
import os
import sys
import warnings

# æ·»åŠ æ¨¡å‹è·¯å¾„
sys.path.append('../model')

import config
import main as main_module
from utils import chinese_tokenizer_load, english_tokenizer_load


def check_environment():
    """æ£€æŸ¥è¿è¡Œç¯å¢ƒ"""
    print("ğŸ” ç¯å¢ƒæ£€æŸ¥")
    print("=" * 50)
    
    # Pythonç‰ˆæœ¬
    print(f"Pythonç‰ˆæœ¬: {sys.version}")
    
    # PyTorchç‰ˆæœ¬
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # CUDAä¿¡æ¯
    if torch.cuda.is_available():
        print(f"CUDAå¯ç”¨: âœ…")
        print(f"CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"GPUæ•°é‡: {torch.cuda.device_count()}")
        
        for i in range(torch.cuda.device_count()):
            gpu_name = torch.cuda.get_device_name(i)
            gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
            print(f"GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
    else:
        print("CUDAå¯ç”¨: âŒ")
    
    print(f"ä½¿ç”¨è®¾å¤‡: {config.device}")
    print()


def check_data():
    """æ£€æŸ¥æ•°æ®æ–‡ä»¶"""
    print("ğŸ“Š æ•°æ®æ£€æŸ¥")
    print("=" * 50)
    
    data_files = [
        ("è®­ç»ƒé›†", config.train_data_path),
        ("éªŒè¯é›†", config.dev_data_path), 
        ("æµ‹è¯•é›†", config.test_data_path)
    ]
    
    for name, path in data_files:
        if os.path.exists(path):
            size = os.path.getsize(path) / 1024 / 1024  # MB
            print(f"{name}: âœ… ({size:.1f}MB) - {path}")
        else:
            print(f"{name}: âŒ æ–‡ä»¶ä¸å­˜åœ¨ - {path}")
            return False
    
    print()
    return True


def check_tokenizers():
    """æ£€æŸ¥åˆ†è¯å™¨"""
    print("ğŸ”¤ åˆ†è¯å™¨æ£€æŸ¥")
    print("=" * 50)
    
    try:
        # ä¸­æ–‡åˆ†è¯å™¨ï¼ˆæºè¯­è¨€ï¼‰
        cn_tokenizer = chinese_tokenizer_load()
        cn_vocab_size = len(cn_tokenizer)
        print(f"ä¸­æ–‡åˆ†è¯å™¨: âœ… (è¯æ±‡è¡¨å¤§å°: {cn_vocab_size})")
        
        # è‹±æ–‡åˆ†è¯å™¨ï¼ˆç›®æ ‡è¯­è¨€ï¼‰
        en_tokenizer = english_tokenizer_load()
        en_vocab_size = len(en_tokenizer)
        print(f"è‹±æ–‡åˆ†è¯å™¨: âœ… (è¯æ±‡è¡¨å¤§å°: {en_vocab_size})")
        
        # æµ‹è¯•åˆ†è¯
        test_cn = "ä½ å¥½ï¼Œä¸–ç•Œï¼"
        test_en = "Hello, world!"
        
        cn_tokens = cn_tokenizer.EncodeAsIds(test_cn)
        en_tokens = en_tokenizer.EncodeAsIds(test_en)
        
        print(f"æµ‹è¯•åˆ†è¯ - ä¸­æ–‡: '{test_cn}' -> {cn_tokens}")
        print(f"æµ‹è¯•åˆ†è¯ - è‹±æ–‡: '{test_en}' -> {en_tokens}")
        
        print()
        return True
        
    except Exception as e:
        print(f"åˆ†è¯å™¨åŠ è½½å¤±è´¥: {e}")
        return False


def check_model_config():
    """æ£€æŸ¥æ¨¡å‹é…ç½®"""
    print("âš™ï¸  æ¨¡å‹é…ç½®")
    print("=" * 50)
    
    print(f"æ¨¡å‹å±‚æ•°: {config.n_layers}")
    print(f"æ¨¡å‹ç»´åº¦: {config.d_model}")
    print(f"æ³¨æ„åŠ›å¤´æ•°: {config.n_heads}")
    print(f"å‰é¦ˆç½‘ç»œç»´åº¦: {config.d_ff}")
    print(f"Dropout: {config.dropout}")
    print(f"æºè¯æ±‡è¡¨å¤§å°: {config.src_vocab_size}")
    print(f"ç›®æ ‡è¯æ±‡è¡¨å¤§å°: {config.tgt_vocab_size}")
    print(f"æ‰¹å¤§å°: {config.batch_size}")
    print(f"æœ€å¤§è½®æ•°: {config.epoch_num}")
    print(f"æ—©åœè½®æ•°: {config.early_stop}")
    print(f"ä½¿ç”¨NoamOpt: {config.use_noamopt}")
    print(f"ä½¿ç”¨æ ‡ç­¾å¹³æ»‘: {config.use_smoothing}")
    print()


def load_dataset_info():
    """åŠ è½½æ•°æ®é›†ä¿¡æ¯"""
    print("ğŸ“ˆ æ•°æ®é›†ä¿¡æ¯")
    print("=" * 50)
    
    try:
        from data_loader import MTDataset
        
        # åˆ›å»ºæ•°æ®é›†
        train_dataset = MTDataset(config.train_data_path)
        dev_dataset = MTDataset(config.dev_data_path)
        test_dataset = MTDataset(config.test_data_path)
        
        print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset):,}")
        print(f"éªŒè¯æ ·æœ¬æ•°: {len(dev_dataset):,}")
        print(f"æµ‹è¯•æ ·æœ¬æ•°: {len(test_dataset):,}")
        
        # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
        print("\næ ·æœ¬ç¤ºä¾‹:")
        for i in range(min(3, len(train_dataset))):
            sample = train_dataset[i]
            print(f"  æ ·æœ¬{i+1}:")
            print(f"    ä¸­æ–‡: {sample[0][:50]}{'...' if len(sample[0]) > 50 else ''}")
            print(f"    è‹±æ–‡: {sample[1][:50]}{'...' if len(sample[1]) > 50 else ''}")
        
        print()
        return True
        
    except Exception as e:
        print(f"æ•°æ®é›†åŠ è½½å¤±è´¥: {e}")
        return False


def estimate_model_size():
    """ä¼°ç®—æ¨¡å‹å¤§å°"""
    print("ğŸ§® æ¨¡å‹å¤§å°ä¼°ç®—")
    print("=" * 50)
    
    try:
        from model.transformer import make_model
        
        # åˆ›å»ºæ¨¡å‹
        model = make_model(
            config.src_vocab_size, config.tgt_vocab_size, config.n_layers,
            config.d_model, config.d_ff, config.n_heads, config.dropout
        )
        
        # è®¡ç®—å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    
        print(f"æ€»å‚æ•°é‡: {total_params:,} ({total_params/1000000:.2f}M)")
        print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,} ({trainable_params/1000000:.2f}M)")
    
        # ä¼°ç®—æ˜¾å­˜å ç”¨ï¼ˆç²—ç•¥ä¼°ç®—ï¼‰
        # å‚æ•° + æ¢¯åº¦ + ä¼˜åŒ–å™¨çŠ¶æ€ + æ¿€æ´»å€¼
        param_memory = total_params * 4 / 1024 / 1024  # MB (float32)
        grad_memory = param_memory  # æ¢¯åº¦
        optimizer_memory = param_memory * 2  # AdamçŠ¶æ€
        activation_memory = config.batch_size * 512 * config.d_model * 4 / 1024 / 1024  # ç²—ç•¥ä¼°ç®—
        
        total_memory = param_memory + grad_memory + optimizer_memory + activation_memory
        
        print(f"ä¼°è®¡æ˜¾å­˜å ç”¨: {total_memory:.1f}MB ({total_memory/1024:.2f}GB)")
        print()
        
        return True
        
    except Exception as e:
        print(f"æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
        return False


def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ ä¸­è¯‘è‹±æ¨¡å‹è®­ç»ƒå¯åŠ¨")
    print("=" * 70)
    print()
    
    # æŠ‘åˆ¶è­¦å‘Š
    warnings.filterwarnings('ignore')
    
    # è®¾ç½®CUDAè®¾å¤‡
    os.environ['CUDA_VISIBLE_DEVICES'] = '0'
    
    # ç¯å¢ƒæ£€æŸ¥
    check_environment()
    
    # æ•°æ®æ£€æŸ¥
    if not check_data():
        print("âŒ æ•°æ®æ£€æŸ¥å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ•°æ®æ–‡ä»¶")
        return
    
    # åˆ†è¯å™¨æ£€æŸ¥
    if not check_tokenizers():
        print("âŒ åˆ†è¯å™¨æ£€æŸ¥å¤±è´¥")
        return
    
    # æ¨¡å‹é…ç½®
    check_model_config()
    
    # æ•°æ®é›†ä¿¡æ¯
    if not load_dataset_info():
        print("âŒ æ•°æ®é›†åŠ è½½å¤±è´¥")
        return
    
    # æ¨¡å‹å¤§å°ä¼°ç®—
    if not estimate_model_size():
        print("âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥")
        return
    
    print("âœ… æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œå¼€å§‹è®­ç»ƒ...")
    print("=" * 70)
    print()
    
    try:
        # å¯åŠ¨è®­ç»ƒ
        main_module.run()
        
    except KeyboardInterrupt:
        print("\nâ¹ï¸ è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        print(f"\nâŒ è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main() 